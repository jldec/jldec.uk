{"cli":true,"dir":".","openBrowser":true,"outputOnly":true,"no-watch":true,"no-sockets":true,"editor":true,"user":"runner","basedir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test","pkgJson":{"name":"cloudflare-pages-test","version":"2021.01.30","description":"copy of jldec.me blog built using jldec/pub-server","repository":"git://github.com/jldec/cloudflare-pages-test.git","main":"pub-config.js","dependencies":{"pub-server":"^2.10.0","pub-theme-pubblog":"^1.4.0"},"scripts":{"start":"pub","generate":"pub -O","preview":"pub -S out"},"license":"MIT"},"pkgName":"cloudflare-pages-test","sources":[{"path":"markdown","writable":true,"name":"markdown","type":"FILE","tmp":"tmp/markdown","watch":{},"fragmentDelim":true,"files":[{"path":"/index.md","text":"---- / ----\nname: JÃ¼rgen Leschner\ntemplate: home\nmultipage: 1\nimage: /images/hotc2015.jpg\n\n---- /#topmenu ----\n"},{"path":"/404.md","text":"---- /404 ----\nname: Not found\nmultipage: 1\nnocrawl: 1\n\n# Sorry! This page is unavailable.\n"},{"path":"/a-web-for-everyone.md","text":"---- /a-web-for-everyone ----\ntitle: A Web for Everyone\nimage: /images/oldstreet.jpg\ndate: 2019-08-08\ntemplate: post\n\n\n## To publish on the web all you __really__ need is a domain name\n\nYou acquire a domain name by filling in a form and paying the domain name registrar. Usually it's a simple process, and doesn't cost a lot.\n\nBy owning the name, you control **what** appears when someone clicks on [https://your-name/](/).\n\nAs the domain owner you also control **who** can publish and who can see the content on your site.\n\nYou can limit access to just your friends, or you can share the domain with your family say, or with everyone in your company, or with all the people in your school.\n\nAnd, the domain owner decides whether to subject visitors to tracking by 3rd parties like Google and Facebook -- or not _!smile-o lg spin_.\n\n## So, why isn't everyone doing this?\n\nThe reality is that it's easier to sign up for a service like Instagram or SnapChat, where you can share your photos with your friends, than it is to buy your own domain name, and choose a hosting provider and a theme, and publish your content yourself. \n\n> Publishing content on the web yourself is simply not convenient -- yet.\n\nFor most people, allowing yourself to be tracked and exposed to advertising is a small price to pay for the convenience of installing an App and connecting with your friends right away.\n\n## But, things are changing\n\nTools for creating content are getting better, and services like [netlify](https://www.netlify.com/) are making it easier to register a domain and publish content on the web.\n\nThese tools and services are targeting developers today, but I think we should be optimistic. Soon these powers will be just as accessible to non-technical users, and people will be communicating privately, without tracking, using their own domains on the web.\n\n> Creating an open Web where anyone can put anything on the Internet is the future.\n\n_from: https://2019.fullstackfest.com/podcast/ (~26:00)_\n\n<iframe src=\"https://player.pippa.io/full-stack-cast/episodes/sara-vieira-gatekeeping-airports-and-making-the-web-for-ever?theme=white&cover=1&latest=1\" frameBorder=\"0\" width=\"100%\" height=\"110px\" allow=\"autoplay\"></iframe>\n\n> _!child_ _!globe 2x_ _!child_\n\n---- #excerpt ----\n\nCreating an open Web where anyone can put anything on the Internet is the future.\n"},{"path":"/about.md","text":"---- /about ----\nname: About me\n\n## JÃ¼rgen Leschner\n\n[@jldec](https://twitter.com/jldec) |\n[github](https://github.com/jldec?tab=repositories) |\n[linkedin](https://www.linkedin.com/in/jldec)\n\n#### Current Favorite\n\n- Physicist: [Andrea Ghez](https://en.wikipedia.org/wiki/Andrea_M._Ghez)\n- Podcast: [Fest & Flauschig](https://open.spotify.com/show/1OLcQdw2PFDPG1jo3s0wbp)\n- Web platform: [Cloudflare Workers](https://workers.cloudflare.com/)\n- UI framework: [Svelte](https://svelte.dev/examples#hello-world)\n\n#### Work Sampling\n\n- word processor: MIT course 6.170 project (CLU) - 1984\n- touchscreen + videodisc authoring: MIT Media Lab (Logo, MagicL) - 1986\n- word processor: MS Word for DOS (x86 ASM and pCode, SNOBOL4) - 1987\n- word processor: MS Word for Windows v1.0a (C++) - 1990\n- database application: POINT (Novell Btrieve) - 1992\n- publishing application: IBM PC Catalog (LotusScript, FrameMaker) - 1995\n- my first commercial [website](https://web.archive.org/web/19961222064651/https://www.fmctraining.com/) - 1996\n- web publishing: Siebel Interactive briefings (HTML & tSQL) - 1998\n- dynamic html front end: Collego Catalog UI - (IE4 JScript) - 1999\n- non-relational database: Collego Catalog DB - binary relations (C++) - 1999\n- same [website](https://web.archive.org/web/20011031190800/https://www.fmctraining.com/) - 2001\n- non-relational database: Monadix XML DB (C# .NET) - 2002\n- non-relational database: PiStore (C++) - 2003\n- consumer publishing app: Pi Photo (XQuery) - 2006\n- same [website](https://web.archive.org/web/20081218114833/https://www.fmctraining.com/) - 2008\n- cloud infrastructure: EMC Atmos Compute Service (Java) - 2008\n- cloud infrastructure: CloudFoundry S3 emulator (node.js) - 2011\n- markdown site generator and editor: [pub-server](https://github.com/jldec/pub-server) (node.js, javascript) - 2014\n- same [website](https://www.fmctraining.com/) - 2016\n- static site hosting: [GitHub Pages](https://pages.github.com) - 2016\n- serverless on Kubernetes: [riff](https://projectriff.io) - 2017\n\n---- #excerpt ----\n\nWork sampling\n"},{"path":"/calling-rust-from-a-cloudflare-worker.md","text":"---- /calling-rust-from-a-cloudflare-worker ----\ntitle: Calling Rust from a Cloudflare Worker\nimage: images/moonbird.jpg\ndate: 2021-02-14\ntemplate: post\n\n## WebAssembly and Rust\n\nFrom the [WebAssembly spec](https://webassembly.github.io/spec/core/intro/introduction.html):\n\n> WebAssembly (abbreviated wasm) is a safe, portable, low-level code format designed for efficient execution and compact representation.\n\nWebAssembly is the first new runnable format supported by all major browsers. It is also showing promise as a standardized way to deploy code to edge environments.\n\nRust is a popular language for writing code compiled to WebAssembly. This is not just because of Rust's minimal runtime needs, but also because of its community and [tools](https://jldec.me/forays-from-node-to-rust#first-impressions).\n\n## Cloudflare Workers\n\n[Cloudflare Workers](https://workers.cloudflare.com/) offers a low-cost, low-latency, serverless platform, making it an ideal complement for statically generated websites. Cloudflare Workers use [V8](https://github.com/v8/v8#readme), the same open source JavaScript engine from Google which is used in [Node.js](https://nodejs.org/en/about/) and [Deno](https://deno.land/).\n\nThe [APIs](https://developers.cloudflare.com/workers/runtime-apis) available to Cloudflare Workers are very limited - Unlike Node, there is no module loader and no access to the host platform.\n\n> Workers handle requests from the Web,  \n> and they can call other Web services.\n\nWhile the default JavaScript is already quite efficient, workers can also run WebAssembly.\n\n## Creating the worker\n\nThis article demonstrates how to build a worker which calls a wasm [function](https://github.com/jldec/shortscale-rs) written in Rust.\n\nThe Cloudflare UI makes it easy to create new workers, but adding WebAssembly requires [API calls](https://api.cloudflare.com/#worker-script-upload-worker) with wasm [Resource Bindings](https://developers.cloudflare.com/workers/platform/scripts#resource-bindings). Since this part of the API is not well documented, using the **wrangler** CLI is easier.\n\n[Install wrangler](https://developers.cloudflare.com/workers/cli-wrangler/install-update) and authenticate with `wrangler login`. Then run the following:\n\n```sh\n$ wrangler generate wasm-worker -t rust\n```\n```\nðŸ”§   Creating project called `wasm-worker`...\nâœ¨   Done! New project created ./wasm-worker\nðŸ•µï¸  You will need to update the following fields in the created wrangler.toml file before continuing:\nðŸ•µï¸  You can find your account_id in the right sidebar of your account's Workers page, and zone_id in the right sidebar of a zone's overview tab at https://dash.cloudflare.com\n- account_id\n```\n\nThis creates a directory called `wasm-worker` populated with files from [github.com/cloudflare/rustwasm-worker-template](https://github.com/cloudflare/rustwasm-worker-template/tree/72d390bf22983d43a1da3681faa093874fa32837).\n\n## wrangler dev\n\nYou can now call `wrangler dev` to build and run the worker.\n\n_Note_: There is no need to run 'wasm-pack' as suggested by the project README, and as of wrangler [v1.18.0](https://github.com/cloudflare/wrangler/releases/tag/v1.18.0), your 'account _id' will be auto-inferred by wrangler, if omitted from wrangler.toml.\n\n```\n$ wrangler dev\nðŸŒ€  Compiling your project to WebAssembly...\n[INFO]: ðŸŽ¯  Checking for the Wasm target...\n[INFO]: ðŸŒ€  Compiling to Wasm...\n...\n   Compiling wasm-worker v0.1.0\n    Finished release [optimized] target(s) in 12.43s\n[INFO]: â¬‡ï¸  Installing wasm-bindgen...\n[INFO]: Optimizing wasm binaries with `wasm-opt`...\n[INFO]: Optional fields missing from Cargo.toml: 'description', 'repository', and 'license'. These are not necessary, but recommended\n[INFO]: âœ¨   Done in 13.03s\n[INFO]: ðŸ“¦   Your wasm pkg is ready to publish at wasm-worker/pkg.\nðŸ’  watching \"./\"\nðŸ‘‚  Listening on http://127.0.0.1:8787\n```\nNow browse to http://127.0.0.1:8787\n\n!['Hello wasm-worker!' appears in the browser](/images/hello-wasm-worker.png)\n\n## Modifications\n\nFor learning purposes, I pared the code down and pushed it to [jldec/wasm-worker](https://github.com/jldec/wasm-worker).\n\n- Removed unused files: `.appveyor.yml`, `.travis.yml`, `.cargo-ok`\n- Removed `worker/metadata_wasm.json` - no longer used by wrangler\n- Removed optional libraries `console_error_panic_hook`, `wee_alloc`, and `cfg-if`\n- Updated version of `wasm-bindgen`\n- Filled in `description`, `license`, and `repository` in `Cargo.toml`\n- Added `Cargo.lock` to `.gitignore`\n- Rewrote the README\n\nI added the [shortscale](https://crates.io/crates/shortscale) crate, and changed `src/lib.rs`.\n\n```rust\nuse shortscale::shortscale;\n\n#[wasm_bindgen]\npub fn numwords(num: u64) -> String {\n    return shortscale(num);\n}\n```\n\nThis is called from the `worker.js`.\n\n```js\n// Return JSON using query param n.\nasync function handleRequest(request) {\n\n  // pick up #[wasm_bindgen] exports from ../src/lib.rs\n  const { numwords } = wasm_bindgen;\n\n  // `wasm` binding name is auto-generated by wrangler\n  await wasm_bindgen(wasm);\n\n  let hello = 'from wasm-worker';\n  let url = new URL(request.url);\n  let n = url.searchParams.get('n');\n  let words;\n\n  try {\n    words = numwords(BigInt(n));\n  }\n  catch (e) {\n    words = 'undefined';\n  }\n\n  return new Response(JSON.stringify({ hello, n, words }),\n    {\n      status: 200,\n      headers: { \"Content-Type\": \"application/json; charset=utf-8\" }\n    }\n  );\n}\n```\n\n## wrangler publish\n\nFinally, I added a route and zone ID to my wrangler.toml and called `wrangler publish`\n\n```\n$ wrangler publish\nðŸŒ€  Compiling your project to WebAssembly...\n[INFO]: ðŸŽ¯  Checking for the Wasm target...\n[INFO]: ðŸŒ€  Compiling to Wasm...\n    Finished release [optimized] target(s) in 0.03s\n[INFO]: â¬‡ï¸  Installing wasm-bindgen...\n[INFO]: Optimizing wasm binaries with `wasm-opt`...\n[INFO]: âœ¨   Done in 0.47s\n[INFO]: ðŸ“¦   Your wasm pkg is ready to publish at wasm-worker/pkg.\nâœ¨  Build succeeded\nâœ¨  Successfully published your script to\n jldec.net/wasm-worker* => stayed the same\n https://wasm-worker.jldec.workers.dev\n```\n\nYou can run the result at https://jldec.net/wasm-worker?n=123456789012345678 - Round-trip response times in my area average under 30ms.\n\n![hello\t\"from wasm-worker\" n \"123456789012345678\" words\t\"one hundred and twenty three quadrillion four hundred and fifty six trillion seven hundred and eighty nine billion twelve million three hundred and forty five thousand six hundred and seventy eight\"](/images/worker-request.png)\n\n\n##  ðŸ¦€ Keep ðŸ¦€ Digging ðŸ¦€\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/calling-rust-from-a-cloudflare-worker-17b4)_\n\n---- #excerpt ----\n\nHow to build a Worker which calls a WebAssembly library written in Rust.\n"},{"path":"/extracting-an-esm-module-from-a-deno-script.md","text":"---- /extracting-an-esm-module-from-a-deno-script ----\ntitle: Extracting an ESM module from a Deno script\nimage: images/persewide.jpg\ndate: 2021-03-21\ntemplate: post\nmetap-og;title: Extracting an ESM module from a Deno script\nmetap-og;image: https://jldec.me/images/persewide.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/extracting-an-esm-module-from-a-deno-script\nmetap-og;description: Will the NPM ecosystem evolve to support nested ESM modules?\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Extracting an ESM module from a Deno script\nmeta-twitter;description: Will the NPM ecosystem evolve to support nested ESM modules?\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/persewide.jpg\nmeta-twitter;image;alt: Sunny day in Cambridge UK\n\nThis is another followup to my recent post about [Getting Started with Deno](/getting-started-with-deno).\n\nI thought it would make sense to extract the crawler code into its own [ESM module](/migrating-from-cjs-to-esm) so that it can also be used with Node.js or in the browser.\n\nThe resulting [API](https://github.com/jldec/deno-hello/blob/main/scanurl.mjs#L18) is a bit ugly because it expects parse5 and fetch as parameters, but it works  .\n\n```js\n/**\n * @param {URL} rootURL\n * @param {boolean} noRecurse\n * @param {boolean} quiet\n * @param {function} parse5 - transitive dependency\n * @param {function} fetch - native or npm package\n * @param {Object} fetchOpts options passed to fetch - optional\n * @returns {Object} map of url -> { url, status, in, [error] }\n */\nexport default async function scanurl(rootURL, noRecurse, quiet, parse5, fetch, fetchOpts) {\n```\n\n## Calling the ESM module from the browser\n\nYou can try running the module from inside your own browser at https://deno-hello.jldec.me/.\n\n[![Screenshot of https://deno-hello.jldec.me](/images/deno-hello.jldec.me.png)](https://deno-hello.jldec.me/)\n\nThe page shows how to import the module from an inline `<script type=\"module\">`.\n\n```html\n<script type=\"module\" id=\"code\">\nimport scanurl from './scanurl.mjs';\nimport parse5 from 'https://cdn.skypack.dev/parse5';\n...\n</script>\n```\n\nNote that the usual browser CORS restrictions also apply to ESM modules, and to fetch() calls. In this case 'scanurl' is imported using a relative path on the same origin, and 'parse5' is imported using https://www.skypack.dev/.\n\n## Using the scanode ESM module with Node\n\nI have published [scanode](https://www.npmjs.com/package/scanode) as a package on npm. If you have Node, you can run it with 'npx' or install it using 'npm install'.\n\n```\n$ npx scanode https://jldec.me\nnpx: installed 3 in 0.987s\nparsing /\n...\n14 pages scanned.\nðŸŽ‰ no broken links found.\n```\n\nYou can also call the module API from your own code as in [node_example/test-scan.js](https://github.com/jldec/deno-hello/blob/main/node_example/test-scan.js).\n\n```js\nimport fetch from 'node-fetch';\nimport parse5 from 'parse5';\nimport scanode from 'scanode';\n\nconst result = await scanode(\n  new URL('https://jldec.me'),\n  false, // noRecurse\n  false, // quiet\n  parse5,\n  fetch\n);\n```\n\nNotice the imports for 'parse5' and 'node-fetch'. These are included as dependencies in the [package.json](https://github.com/jldec/deno-hello/blob/main/package.json#L9) for scanode.\n\n```json\n{\n  \"name\": \"scanode\",\n  \"version\": \"2.0.1\",\n  \"description\": \"ESM module - crawls a website, validating that all the links on the site which point to the same orgin can be fetched.\",\n  \"main\": \"scanurl.mjs\",\n  \"bin\": {\n    \"scanode\": \"./bin/scanode.mjs\"\n  },\n  \"dependencies\": {\n    \"node-fetch\": \"^2.6.1\",\n    \"parse5\": \"^6.0.1\"\n  }\n  ...\n```\n\n## So what's wrong with this picture?\n\nAs discussed [before](/migrating-from-cjs-to-esm), the NPM ecosystem predates ESM modules, so the two worlds don't play very nicely together. Node.js programs cannot easily load ESM modules which are not in NPM. Meanwhile, browsers know nothing about package.json or the node_modules directory.\n\nWhen ESM modules depend on other modules, they use 'import' statements with a URL or relative path. Node.js expects those sub-modules to be referenced by their NPM package names.\n\nThe result is that modules which depend on other modules are not portable between the two worlds, without an additional transformation step, or maybe an [import map](https://caniuse.com/import-maps).\n\nAnd this is why, for now, the API above expects the `parse5` module dependency as a parameter.\n\n> The big question is whether the NPM ecosystem will evolve to support nested ESM modules, or whether some other organization with a workable trust model will emerge to replace it.\n\nWhere there's a problem, there's an opportunity!\n\n# ðŸš€\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/extracting-an-esm-module-from-a-deno-script-28il)_\n\n---- #excerpt ----\n\nHow to extract an [ESM module](/migrating-from-cjs-to-esm) so that it can also be used with Node.js or in the browser.\n\nWill the NPM ecosystem evolve to support nested ESM modules, or will some other organization, with a workable trust model, emerge to replace it?\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"path":"/first-impressions-of-the-new-github-projects-beta.md","text":"---- /first-impressions-of-the-new-github-projects-beta ----\ntitle: First impressions of the new GitHub Projects Beta\nimage: images/red-autumn-leaves.jpg\ndate: 2021-10-31\ntemplate: post\nmetap-og;title: First impressions of the new GitHub Projects Beta\nmetap-og;image: https://jldec.me/images/red-autumn-leaves.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/first-impressions-of-the-new-github-projects-beta\nmetap-og;description: Using the new GitHub Issues and Project tables to break down and prioritize issues\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: First impressions of the new GitHub Projects Beta\nmeta-twitter;description: Using the new GitHub Issues and Project tables to break down and prioritize issues\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/red-autumn-leaves.jpg\nmeta-twitter;image;alt: Autumn leaves in Cambridge UK\n\n\n## Issues with GitHub issues\n\nGitHub issues have historically provided a simple yet powerful way to track work in your GitHub repositories. Each issue includes a description, assignee(s), and a timeline with a comment thread and automatically-generated references to related issues and PRs. Issues can be categorized using labels, and the issues list can be searched or filtered in many ways.\n\nBut as projects get larger and more complex, working with issues also has its challenges.\n\n- How to turn large \"Epic\" issues into smaller issues?\n- How to prioritize and organize issues into iterations?\n- How to track issues across multiple repositories?\n- How best to incorporate community contributions and other feedback?\n\nGitHub has incrementally tried to address some of these challenges. \n\n- [Task lists](https://docs.github.com/en/issues/tracking-your-work-with-issues/about-task-lists) add convenient checkboxes to markdown lists in issue descriptions.\n\n- [Milestones](https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/about-milestones) provide a simple way to collect and prioritize issues within a repo. \n\n- [Projects](https://docs.github.com/en/issues/organizing-your-work-with-project-boards/managing-project-boards) started as single-repo kanban boards, with issues or notes moving vertically or sideways. They later acquired cross-repo and limited automation capabilities.\n\n- [Issue templates](https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/about-issue-and-pull-request-templates) help contributors to include specific information in an issue.\n\n## The new GitHub Issues (Beta)\n\n> With their Issues announcement in June, GitHub signalled a less-incremental approach.\n\nThe [announcement](https://github.blog/changelog/2021-06-23-whats-new-with-github-issues/) included two parts\n\n1. A powerful new way to [create](https://docs.github.com/en/issues/trying-out-the-new-projects-experience/creating-a-project) and [group issues](https://docs.github.com/en/issues/trying-out-the-new-projects-experience/customizing-your-project-views) into projects.\n2. Ways to grow from ideas expressed as text, into collections of issues.\n\n[More announcements](https://github.blog/changelog/label/issues/) have followed, and last week the [Beta opened up](https://github.blog/changelog/2021-10-27-the-new-github-issues-public-beta/) to all users.\n\n## Beta Projects\n\n[Project tables](https://docs.github.com/en/issues/trying-out-the-new-projects-experience/about-projects) are spreadsheet-like views where each row is a real or draft issue. \n\n[![Screenshot of GitHub project table](/images/gh-project-table.png)](https://github.com/orgs/github/projects/4247/views/7)\n\nRows (issues) can be grouped by field value. This includes custom fields whose values are maintained in the project instead of on issues in a repo. Maintaining custom field data inside projects is key to their power.\n\n> Rather than polluting your issues with all possible categories of tags, new categories can be scoped inside a project.\n\nSince Beta Projects also support kanban views, I expect them to replace the existing Projects once they reach feature parity.\n\n## From text to issues\n\nAuto-creating issues from [task lists](https://docs.github.com/en/issues/tracking-your-work-with-issues/about-task-lists#about-issue-task-lists) makes it easier to break down \"epic\" issues into smaller sub-issues. The task list item is checked off when the issue is resolved, and the child-issue links to the parent-issue with the task list.  \n\n[![Screenshot of GitHub issue with a task list](/images/gh-task-list.png)](https://github.com/gitpod-io/gitpod/issues/3065)\n\nSimilarly, [draft issues](https://docs.github.com/en/issues/trying-out-the-new-projects-experience/creating-a-project#adding-items-to-your-project) which are simply rows entered as text in a project table, can also be [converted](https://docs.github.com/en/issues/trying-out-the-new-projects-experience/creating-a-project#converting-draft-issues-to-issues) into issues. \n\n[![Screenshot of converting draft issue to issue in GitHub project table](/images/gh-project-table-convert-to-issue.png)](https://github.com/orgs/github/projects/4247/views/7)\n\n> This makes task lists and project tables two convenient ways to brainstorm ideas, and break them down into smaller issues.\n\n## What's next\n\nThe GitHub public roadmap features a [planning view](https://github.com/orgs/github/projects/4247/views/7) specific to issues. \n\nA number of informative talks at the recent [GitHub Universe 2021](https://www.githubuniverse.com/2021/) also provided hints about what the team is planning.\n\n> Remember that the new projects and issues features are still in Beta.\n\n\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/first-impressions-of-the-new-github-projects-beta-nca)_\n\n---- #excerpt ----\n\nProject tables are spreadsheet-like views where each row is a real or a draft issue. Beta Projects also support kanban views. Task lists and draft issues can be converted into real issues."},{"path":"/first-steps-using-cloudflare-pages.md","text":"---- /first-steps-using-cloudflare-pages ----\ntitle: First steps using Cloudflare Pages\nimage: images/small-clouds.jpg\ndate: 2021-01-31\ntemplate: post\n\n## jldec.me\n\nMy personal blog, [jldec.me](https://jldec.me), is hosted on [Netlify](https://netlify.com). Whenever I push markdown to GitHub, Netlify runs a build and publishes the HTML.\n\nCloudflare recently [announced](https://blog.cloudflare.com/cloudflare-pages/) a similar offering called [Cloudflare Pages](https://pages.cloudflare.com/). I was lucky enough to be given access to the Beta.\n\nUnlike [Netlify](https://www.netlify.com/pricing/#features), Cloudflare Pages [does not meter](https://pages.cloudflare.com/#pricing) request traffic. This opens the door for use-cases like CDN hosting of open source [ESM modules](/migrating-from-cjs-to-esm) ðŸ¤”.\n\n## Cloudflare Pages (Beta)\n\nThis is a walkthrough of setting up [jldec.eu](https://jldec.eu), a copy of [jldec.me](https://jldec.me), on Cloudflare Pages.\n\nIf you have access to Cloudflare Pages, you will see this button when you login to Cloudflare.\n\n![Cloudflare Pages Beta button on dashboard](/images/cf-pages-beta.png)\n\nThe Pages button opens your Pages projects -- of which there are none at first -- and a button to `Create a project`.\n\n![Pages - Create a project](/images/cf-pages-create-a-project.png)\n\n This opens the GitHub form for granting repo access to the 'Cloudflare Pages' GitHub app. (_Look for it later in your [GitHub Settings](https://github.com/settings/installations) to add more repos, or to revoke access._)\n\n![Authorize Cloudflare Pages app on GitHub](/images/cf-pages-github-app.png)\n\nBack on Cloudflare, you can choose the repo for your new Cloudflare Pages project.  \n[cloudflare-pages-test](https://github.com/jldec/cloudflare-pages-test) is a copy of my markdown source repo from [jldec.me](https://jldec.me).\n\n![Select repo for the Cloudflare Pages project](/images/cf-pages-select-repo.png)\n\nIn the configuration form, I provided branch name, build command, and output directory.  \nThe project name defaults to the repo name.\n\n![Configure the build command and output directory](/images/cf-pages-configure-build.png)\n\nSubmitting the form, triggers the first build and shows the log.\n\n![First build and deploy showing log](/images/cf-pages-build-log.png)\n\nThe project page also has a section for configuring custom domains. I used my own cloudflare-hosted domain [jldec.eu](https://jldec.eu). The [docs](https://developers.cloudflare.com/pages/getting-started#add-a-custom-cname-record) can be a little confusing here. My CNAME points to `cloudflare-pages-test.pages.dev` not `custom.pages.dev`.\n\n![Cloudflare Pages custom domain](/images/cf-pages-custom-domain.png)\n\nYou can visit the deployed site at [jldec.eu](https://jldec.eu). ðŸ‡ªðŸ‡º  \nSubsequent commits to this GitHub [repo](https://github.com/jldec/cloudflare-pages-test) will trigger a fresh build and re-deploy.\n\n![More deployments](/images/cf-pages-deployments.png)\n\n## GitHub Pages\n\nFor comparison, I set up [jldec.uk](https://jldec.uk), another copy of [jldec.me](https://jldec.me) using [GitHub Pages](https://pages.github.com).\n\nFirst I created a new jldec.uk [repo](https://github.com/jldec/jldec.uk/) to host the GitHub Pages site. Since the output includes javascript bundles, fonts, etc., I prefer to keep it separate from the source.\n\nI pushed the first generated website to this repo manually, using the output of a local build. The empty `.nojekyll` file is important to avoid a Jekyll build on GitHub.\n\n![GitHub Pages repo](/images/gh-pages-repo.png)\n\nNext I configured GitHub Pages in the repo settings ([...looks familiar ðŸ˜ƒ](https://github.blog/2016-08-17-simpler-github-pages-publishing/))\n\n![GitHub Pages settings](/images/gh-pages-settings.png)\n\nYou can visit the deployed site at [jldec.uk](https://jldec.uk). ðŸ‡¬ðŸ‡§  \n\nFinally I set up [GitHub Actions](https://github.com/jldec/cloudflare-pages-test/blob/main/.github/workflows/generate.yaml) to auto-build and auto-deploy the website when the source changes. This is triggered on push, does a checkout of both repos, and commits the new generated output, only when there are actual changes.\n\n```yaml\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\njobs:\n  generate:\n    runs-on: ubuntu-latest\n    env:\n      JLDEC_UK: TRUE\n\n    steps:\n    - name: checkout source repo\n      uses: actions/checkout@v2\n\n    - name: checkout destination repo under ./out\n      uses: actions/checkout@v2\n      with:\n        repository: jldec/jldec.uk\n        token: ${{ secrets.GH_TOKEN }}\n        path: out\n\n    - name: generate output\n      run: |\n        npm ci\n        rm -r out/*\n        npm run generate\n        cd out\n        git config user.email \"jldec@ciaosoft.com\"\n        git config user.name \"cloudflare-pages-test generate action\"\n        git status\n        git add -A\n        if ! git diff-index --quiet HEAD ; then git commit -m 'https://github.com/jldec/cloudflare-pages-test/actions/runs/${{ github.run_id }}' && git push ; fi\n        echo done\n```\nNow every push triggers a new build and re-deploy.\n\n![GitHub Pages builds using GitHub Actions](/images/gh-pages-builds.png)\n\nPreserving the HTML site in git is useful for all kinds of reasons. E.g. here is part of a diff from a recent [commit](https://github.com/jldec/jldec.uk/commit/0efb3e73ea2de797f9201b69803c70299be05a28).\n\n![GitHub Pages diff](/images/gh-pages-diff.png)\n\n## Conclusions\n\nThe developer experience of hosting a site with CloudFlare Pages is very similar to Netlify.\n\nThe Cloudflare Pages Beta does not yet support redirects and functions, but those are expected with the integration of [Cloudflare Workers](https://workers.cloudflare.com).\n\nAutomating builds and deploys onto GitHub Pages is more work, and requires knowledge of GitHub Actions if you're not using Jekyll. There are other gotchas with GitHub Actions if you want to support concurrent builds or preview builds.\n\n> The performance of all 3 platforms is excellent since they all serve static files from a CDN  \n> ðŸƒâ€â™€ï¸\n\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/first-steps-using-cloudflare-pages-40gp)_\n\n---- #excerpt ----\n\nThis is a walkthrough of my first Cloudflare Pages (Beta) site, and a comparison with GitHub Pages.\n\n\n"},{"path":"/forays-from-node-to-rust.md","text":"---- /forays-from-node-to-rust ----\ntitle: Forays from Node to Rust\nimage: images/fog.jpg\ndate: 2021-01-10\ntemplate: post\n\n## Why Rust?\n\nA couple of years ago I picked up the excellent [Programming Rust](https://www.oreilly.com/library/view/programming-rust/9781491927274/) book.\n\nReading how the Rust compiler enforces memory safety and avoids data-races reminded me of the AHA! moment, when I learned how [Node.js](https://nodejs.org/en/about/) makes concurrency accessible to JavaScript developers, without the synchronization headaches of multi-threaded servers.\n\nBut there's more. Rust programs have a very minimal runtime - no garbage collector or class loader. This makes Rust ideal for constrained environments like embedded systems or edge compute platforms - so watch [this](https://github.com/oxidecomputer) [space](https://github.com/bytecodealliance).\n\n## First impressions\n\nThis article covers the experience of buiding my first Rust crate.\n\nThe [shortscale-rs](https://github.com/jldec/shortscale-rs) library tries to replicate [shortscale](https://github.com/jldec/shortscale), a small JavaScript module with just one function which converts numbers to English words.\n\nThe [Rust ecosystem](https://www.rust-lang.org) has produced an absolutely awesome array of tools and documentation.\n\nTo get started:\n- Install Rust [using rustup](https://www.rust-lang.org/tools/install).\n- Run 'rustup update' whenever there is a new [Rust release](https://github.com/rust-lang/rust/releases).\n\nThose steps also take care of 'cargo', the Rust build tool.\n\n![Image showing cargo commands](/images/cargo.png)  \n_https://www.rust-lang.org/learn/get-started_\n\n## VS Code\n\nI followed the [recommendations](https://jason-williams.co.uk/debugging-rust-in-vscode) of Jason Williams to install [Rust Analyzer](https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer) for VS Code instead of the default Rust extension. You'll also need [CodeLLDB](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) for debugging.\n\n![VS Code showing Rust program](/images/vs-code-rust.png)  \nI particularly like the ability to run doctests directly in the VS Code terminal.\n\n## Rust String and str\n\nIn **JavaScript** building strings is straightforward. Simply use `+` to concatenate any string to any other string. Empty strings being [falsy](https://developer.mozilla.org/en-US/docs/Glossary/Falsy) helps to write very compact logic.\n\nThe example below from [shortscale.js](https://github.com/jldec/shortscale/blob/main/shortscale.js#L96) behaves like the built-in [Array.join](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/join), except that it avoids repeating separators by ignoring empty strings.\n\n```js\n// concatenate array of strings, separated by sep, ignoring '' values\nfunction concat(strings, sep) {\n  return strings.reduce((s1, s2) => s1 + (s1 && s2 ? sep : '') + s2, '')\n}\n```\n\nHere's my [first attempt](https://github.com/jldec/shortscale-rs/blob/main/src/extra.rs#L374) to do something similar in **Rust**.\n\n```rust\ntype Strvec = Vec<&'static str>;\n\n// concatenate 2 Strvec's, separated with \"and\" if both have length\nfn concat_and(v1: Strvec, v2: Strvec) -> Strvec {\n    match (v1.len(), v2.len()) {\n        (_, 0) => v1,\n        (0, _) => v2,\n        (_, _) => [v1, vec![\"and\"], v2].concat(),\n    }\n}\n```\n\n'Why Strvec?', you might ask. In Rust, the primitive string type, used for string literals, is a [str](https://doc.rust-lang.org/nightly/std/primitive.str.html). My first thought was that shortscale-rs should manipulate collections of str's. So, instead of using [String](https://doc.rust-lang.org/nightly/std/string/struct.String.html) concatenation, I put str's into [Vec](https://doc.rust-lang.org/nightly/std/vec/struct.Vec.html)'s.\n\nNotice the elegant [match](https://doc.rust-lang.org/rust-by-example/flow_control/match.html) syntax - one of my favorite Rust language features. The compiler ensures that the 'arms' of the match cover all possible inputs. The result is both readable and concise. The '_' is shorthand for any value.\n\n> Performance does not matter,  \n> until it absolutely does.  \n> [@matteocollina](https://twitter.com/matteocollina/status/1260887018617352192?s=20)\n\n## Benchmarks\n\nThe measured [performance](https://github.com/jldec/shortscale-rs#extra) was, well, an eye-opener! ~4459ns per [shortscale_vec_concat](https://docs.rs/shortscale/1.3.2/src/shortscale/extra.rs.html#314-336) call in Rust, compared to ~1342ns for the equivalent in Node.js.\n\n[cargo bench](https://github.com/jldec/shortscale-rs/blob/main/benches/bench-shortscale.rs)\n```\nshortscale                          251 ns/iter (+/- 18)\nshortscale_string_writer_no_alloc   191 ns/iter (+/- 11)\nshortscale_str_push                 247 ns/iter (+/- 22)\nshortscale_vec_push                 363 ns/iter (+/- 26)\nshortscale_display_no_alloc         498 ns/iter (+/- 21)\nshortscale_vec_concat              4459 ns/iter (+/- 344)\nshortscale_string_join             5549 ns/iter (+/- 378)\n```\n\n[npm run bench](https://github.com/jldec/shortscale/blob/main/test/bench.js)\n```\nshortscale                         1342 ns/iter\n```\n\nClearly the v8 JavaScript engine in Node.js is working very hard to make string manipulation efficient.\n\n## Learn & Iterate\n\nThe next thing I tried was to replace the Vec collections with simple Strings, creating and returning those from each function in the Rust program. This is [shortscale_string_join](https://docs.rs/shortscale/1.3.2/src/shortscale/extra.rs.html#389-406). You should see from the benchmark, that its performance was _even worse_. Clearly I was doing something wrong.\n\nFast forward to the [current implementation](https://docs.rs/shortscale/1.3.2/src/shortscale/shortscale.rs.html#46-61), which mutates a pre-allocated String rather than calling functions which create and return new Strings.\n\n> The result is significantly faster than JavaScript.\n\nI still have a lot to learn, but this exercise was a great way to start building an intuition for Rust development and the performance of Rust primitives.\n\n> _!cogs 3x_\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/forays-from-node-to-rust-3fk1)_\n\n---- #excerpt ----\n\nThis article covers the experience of buiding my first Rust crate.\n\n"},{"path":"/fun-with-vercel.md","text":"---- /fun-with-vercel ----\ntitle: Fun with Vercel\nimage: images/library.jpg\ndate: 2021-02-07\ntemplate: post\n\n## Vercel\n\nLast week I wrote about [](first-steps-using-cloudflare-pages).\n\n[Vercel](https://vercel.com/home) is another leader in the trend toward static hosting, serverless, and [edge compute](/why-serverless-at-the-edge).\n\n## [jldec.fun](https://jldec.fun/fun-with-vercel) ðŸ¤ª\n\nThis is a walkthrough of how I deployed [jldec.fun](https://jldec.fun/) using the [Vercel platform](https://vercel.com/docs).\n\nMarkdown source files live in the same repo on [GitHub](https://github.com/jldec/cloudflare-pages-test) as before.\n\nThe Vercel signup starts a New Project workflow _(this could be simplified.)_\n\n![Vercel new project workflow with link to GitHub](/images/vercel-new-project.png)\n\nI clicked on 'Continue with GitHub', and then chose the first option in the dropdown below.\n\n![Vercel add GitHub account dropdown](/images/vercel-add-github-account.png)\n\nThis led to the GitHub form for granting repo access to the 'Vercel' GitHub app. (_Look for it later in your [GitHub Settings](https://github.com/settings/installations) to add more repos, or to revoke access._)\n\n![Authorize Vercel app on GitHub](/images/vercel-github-app.png)\n\nBack on Vercel, I selected my repo and used my 'Personal Account' scope. Team scope requires a paid plan.\n\n![Select scope for the new project](/images/vercel-select-project-scope.png)\n\nI continued with the default (root) project directory within the repo.\n\n![Select project direcotry inside repo](/images/vercel-select-directory.png)\n\nIn the last form of the Import Project flow, Project Name defaults to the name of the repo, so I changed that to 'jldec-fun'. I also configured a build command and an output directory.\n\nI didn't really need encrypted [environment variables](https://vercel.com/docs/environment-variables) here, but this step doesn't offer an alternative, so I made a note to change it later.\n\n![Configure the build command and output directory](/images/vercel-configure-build-jldec-fun.png)\n\nSubmitting the form triggered the first build, followed by a nice confetti shower. ðŸŽ‰\n\n![First build and deploy confetti celebration](/images/vercel-confetti-jldec-fun.png)\n\nUnfortunately, visiting the site revealed a problem. Vercel does not automatically serve files stored with an '.html' extension, when a request comes in _without_ any extension.\n\n![Page not found when trying to browse toURL without extension](/images/vercel-without-clean-url-setting.png)\n\nThe fix requires [cleanUrls](https://vercel.com/docs/configuration#project/clean-urls) to be set in the `vercel.json` file at the project root.\n\n```json\n{\n  \"cleanUrls\": true\n}\n```\n\nAdding this setting fixed my 404 problem.\n\nThe same config file would also be useful for other common static hosting requirements like [redirects](https://vercel.com/docs/configuration#project/redirects), HTTP [headers](https://vercel.com/docs/configuration#project/headers), and the treatment of URLs with a [trailing slash](https://vercel.com/docs/configuration#project/trailing-slash).\n\nBack in the well-designed Vercel Settings UI, I was able to change my environment variables to plaintext. I liked the ability to set different values for preview builds.\n\n![Dashboard environment settings](/images/vercel-environment-settings.png)\n\nWhen I added my domain name, the form helpfully recommended configuring an IP address.\n\n![Dashboard domain settings](/images/vercel-domain-setting.png)\n\nOnce the DNS record was in place, the domain configuration changed to 'Valid'.\n\n![Dashboard domain settings fixed](/images/vercel-domain-setting-fixed.png)\n\n> Tada!\n\n[jldec.fun](https://jldec.fun/fun-with-vercel) is deployed from the `main` branch on [GitHub](https://github.com/jldec/cloudflare-pages-test). Commits to any other branch will trigger a preview deployment on a different URL.\n\n![screenshot of jldec.fun](/images/vercel-post.png)\n\n> Static websites are awesome!  \n> _!thumbs-o-up 2x_\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/fun-with-vercel-3b6e)_\n\n---- #excerpt ----\n\nHow I deployed [jldec.fun](https://jldec.fun/) using the [Vercel platform](https://vercel.com/docs).\n"},{"path":"/getting-started-with-deno.md","text":"---- /getting-started-with-deno ----\ntitle: Getting Started with Deno\nimage: images/clouds-trees.jpg\ndate: 2021-02-28\ntemplate: post\n\n## Deno\n\n[Deno](https://deno.land/) is a new JavaScript runtime built on [V8](https://github.com/v8/v8#readme).\n\nDeno uses [ESM](/migrating-from-cjs-to-esm) to load JavaScript modules, and natively supports browser APIs like [fetch()](https://deno.land/manual/runtime/web_platform_apis). Together with its [permissions](https://deno.land/manual/getting_started/permissions), this makes Deno feel more like a scriptable web client, and less like a tool for building web servers.\n\nThe Deno executable is built in Rust. While this may seem like an implementation detail, you could also describe Deno as a tool to embed JavaScript inside Rust programs.\n\nThis article describes my first steps using Deno.\n\n## Getting Started\n\nDeno can be [installed](https://deno.land/manual/getting_started/installation) by copying the release from [GitHub](https://github.com/denoland/deno/releases/latest) to a location on your path. I configured my environment as follows:\n\n```sh\nexport DENO_DIR=~/deno\nexport PATH=${PATH}:${DENO_DIR}/bin\n```\n\nOnce installed, 'deno --version' shows the installed version and 'deno upgrade' upgrades the binary to the latest release. 'deno help' shows usage for other commands.\n\nI recommend installing the Deno [VS Code extension](https://github.com/denoland/vscode_deno#readme) for IDE support and debugging. The [manual](https://deno.land/manual/getting_started/debugging_your_code#vscode) suggested a [launch config](https://github.com/jldec/deno-hello/blob/main/.vscode/launch.json), which works for me, _most_ of the time.\n\n![VS Code Deno extension Debug](/images/deno-debug-dark.png)\n\n## hello.js\n\nHere is [hello.js](https://github.com/jldec/deno-hello/tree/main/hello.js), my first Deno program. You can run it with `deno run hello.js args...`\n\n```js\nconst hello = \"Hello Deno\";\nconsole.log(`${hello} %s hello %o`, new Date(), Deno.args);\n\nconst buf = new TextEncoder().encode(\"-ðŸ¦€-\\n\");\nawait Deno.stdout.write(buf);\nconsole.table(buf);\n```\n\nThe easiest way to write to stdout is by using the built-in console.log().\n\nFor those curious about Deno internals:\n\n- The global `console` object is created in [runtime/js/99_main.js](https://github.com/denoland/deno/blob/v1.7.5/runtime/js/99_main.js#L246).\n- The `console.log()` method lives in [runtime/js/02_console.js](https://github.com/denoland/deno/blob/v1.7.5/runtime/js/02_console.js#L1503).\n- This calls the rust function `core.print` in [core/bindings.rs](https://github.com/denoland/deno/blob/v1.7.5/core/bindings.rs#L277).\n\n[Deno.stdout]() provides a lower level stream interface. Notice the `await` on the promise returned by 'Deno.stdout.write()'.\n\n### Typescript\n\nThe code above is also valid [TypeScript](https://www.typescriptlang.org/). Since Deno includes a built-in TypeScript compiler, you could simply rename hello.js to hello.ts and it would work the same way.\n\nThe Deno [Standard Library](https://deno.land/std) is largely written in TypeScript, as are the declarations (and auto-generated docs) for [built-ins](https://doc.deno.land/builtin/stable), so it helps to know a little TypeScript syntax even if you prefer to write JavaScript.\n\nI find the TypeScript declarations most useful for code completion in VS Code.\n\n## scan.js\n\nIn the spirit of leveraging Deno as a web client, I decided to try building a simple link validator. This requires a 3rd-party library to parse HTML.\n\nI started my search assuming that a popular npm module would be my best bet, even if it wasn't available (yet) in [deno.land/x](https://deno.land/x) which is where library authors can register their GitHub repos to publish deno-compatible ESM modules.\n\nAfter some googling, I landed on [parse5](https://github.com/inikulin/parse5) which enjoys wide usage and offers a simple, low-level tree API at its core.\n\nI had also heard about [Skypack](https://docs.skypack.dev/skypack-cdn/code/deno), a CDN, specifically designed to serve npm packages as ESM modules. A quick search on [skypack.dev](https://www.skypack.dev/) and I had a URL for the parse5 module which works in Deno.\n\nThe code in [scan.js](https://github.com/jldec/deno-hello/blob/main/scan.js) crawls a website, validating that all the links on the site which point to the same origin can be fetched.\n\n```js\nimport parse5 from \"https://cdn.skypack.dev/parse5?dts\";\n\nconst rootUrl = Deno.args[0];\nif (!rootUrl) exit(1, \"Please provide a URL\");\n\nconst rootOrigin = new URL(rootUrl).origin;\n\nconst urlMap = {}; // tracks visited urls\n\nawait checkUrl(rootUrl);\nconst result = Object.entries(urlMap).filter((kv) => kv[1] !== \"OK\");\n\nif (result.length) {\n  exit(1, result);\n} else {\n  exit(0, \"ðŸŽ‰ no broken links found.\");\n}\n\n// recursively checks url and same-origin urls inside\n// resolves when done\nasync function checkUrl(url, base) {\n  base = base || url;\n  try {\n    // parse the url relative to base\n    const urlObj = new URL(url, base);\n\n    // ignore query params and hash\n    const href = urlObj.origin + urlObj.pathname;\n\n    // only process same-origin urls\n    if (!urlMap[href] && urlObj.origin === rootOrigin) {\n      // fetch from href\n      urlMap[href] = \"pending\";\n      const res = await fetch(href);\n\n      // bail out if fetch was not ok\n      if (!res.ok) {\n        urlMap[href] = { status: res.status, in: base };\n        return;\n      }\n\n      urlMap[href] = \"OK\";\n\n      // check content type\n      if (!res.headers.get(\"content-type\").match(/text\\/html/i)) return;\n\n      // parse response\n      console.log(\"parsing\", urlObj.pathname);\n      const html = await res.text();\n      const document = parse5.parse(html);\n\n      // scan for <a> tags and call checkURL for each, with base = href\n      const promises = [];\n      scan(document, \"a\", (node) => {\n        promises.push(checkUrl(attr(node, \"href\"), href));\n      });\n      await Promise.all(promises);\n    }\n  } catch (err) {\n    urlMap[url] = { error: err.message, in: base };\n  }\n}\n\n// return value of attr with name for a node\nfunction attr(node, name) {\n  return node.attrs.find((attr) => attr.name === name)?.value;\n}\n\n// recursive DOM scan\n// calls fn(node) on nodes matching tagName\nfunction scan(node, tagName, fn) {\n  if (node?.tagName === tagName) {\n    fn(node);\n  }\n  if (!node.childNodes) return;\n  for (const childNode of node.childNodes) {\n    scan(childNode, tagName, fn);\n  }\n}\n\nfunction exit(code, msg) {\n  console.log(msg);\n  Deno.exit(code);\n}\n```\n\nThis script is hosted at https://deno-hello.jldec.me/ using [Cloudflare Pages](https://jldec.me/first-steps-using-cloudflare-pages).\n\nTo run it, call `deno run --allow-net SCRIPT URL`. E.g.\n\n```sh\n$ deno run --allow-net https://deno-hello.jldec.me/scan.js https://jldec.me\nparsing /\nparsing /getting-started-with-deno\nparsing /first-steps-using-cloudflare-pages\nparsing /calling-rust-from-a-cloudflare-worker\nparsing /a-web-for-everyone\nparsing /why-serverless-at-the-edge\nparsing /fun-with-vercel\nparsing /migrating-from-cjs-to-esm\nparsing /forays-from-node-to-rust\nparsing /about\nparsing /github-actions-101\nparsing /spring-boot-101\nparsing /why-the-web-needs-better-html-editing-components\nðŸŽ‰ no broken links found.\n```\n\nNOTE: For this first implementation, there is no queueing, so I would not recommend pointing it at large site.\n\n## Compiling\n\nThe deno experience still feels a little rough in places, but one new feature which I really like, is the ability to [compile](https://deno.land/manual/tools/compiler) a script into a self-contained executable.\n\n```sh\n$ deno --unstable compile --allow-net scan.js\nBundle file:./scan.js\nCompile file:./scan.js\nEmit scan\n```\n\nNow I can call `scan` without having to install Deno or remember any special options.\n\n```sh\n$ ./scan\nPlease provide a URL\n\n$ ./scan https://jldec.fun\n...\nðŸŽ‰ no broken links found.\n```\n\n> [![Deno logo](/images/deno-logo.png \".no-border\")](https://deno.land/)\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/getting-started-with-deno-2ie7)_\n\n---- #excerpt ----\n\nThe [Deno](https://deno.land/) executable is built in Rust. While this may seem like an implementation detail, you could also describe Deno as a tool to embed JavaScript inside Rust programs.\n"},{"path":"/getting-started-with-go-part-2-pointers.md","text":"---- /getting-started-with-go-part-2-pointers ----\ntitle: Getting started with Go pointers\nimage: images/ladybug.jpg\ndate: 2021-04-18\ntemplate: post\nmetap-og;title: Getting started with Go pointers\nmetap-og;image: https://jldec.me/images/ladybug.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/getting-started-with-go-part-2-pointers\nmetap-og;description: My experience as a new user of Go pointers\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Getting started with Go pointers\nmeta-twitter;description: My experience as a new user of Go pointers\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/ladybug.jpg\nmeta-twitter;image;alt: Ladybug in Cambridge UK\n\n## Golang\n\nThis is part 2 of my experience as a new user of Go, focusing on the quirks and gotchas of pointers. For installation, testing, and packages, see [Getting started with Go](/getting-started-with-go).\n\nIf you'd like to follow along, and try out out the code in this article, all you need is the Go [playground](https://play.golang.org/p/-UiUJFrloVT) to run the examples.\n\n## Pointers\n\nThe [shortscale](https://github.com/jldec/shortscale-go/blob/main/shortscale.go) package which I covered last time, uses a string [Builder](https://pkg.go.dev/strings#Builder). Here is the example from the Builder docs.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\nfunc main() {\n\tvar b strings.Builder\n\tfor i := 3; i >= 1; i-- {\n\t\tfmt.Fprintf(&b, \"%d...\", i)\n\t}\n\tb.WriteString(\"ignition\")\n\tfmt.Println(b.String())\n}\n```\n\nNotice that `var b` is an instance of the Builder. When you run the code, it will output: _3...2...1...ignition_.\n\n## Pointer receiver methods and interfaces\n\nThe first argument to fmt.Fprintf is `&b`, a [pointer](https://tour.golang.org/moretypes/1) to b. This is necessary, because [fmt.Fprintf](https://pkg.go.dev/fmt#Fprintf) expects an [io.Writer](https://pkg.go.dev/io#Writer) interface.\n\n```go\ntype Writer interface {\n\tWrite(p []byte) (n int, err error)\n}\n```\n\nThe [Builder.Write](https://pkg.go.dev/strings#Builder.Write) method matches the io.Writer interface. Notice the pointer syntax in the method receiver after the `func` keyword.\n\n```go\nfunc (b *Builder) Write(p []byte) (int, error)\n```\n\nI was tempted to replace `Fprintf(&b, ...)` with `Fprintf(b, ...)`, to make it more consistent with the `b.WriteString()` and `b.String()` further down, but doing this causes the compiler to complain:\n\n_\"cannot use b (type strings.Builder) as type io.Writer in argument to fmt.Fprintf:\nstrings.Builder does not implement io.Writer (**Write method has pointer receiver**)\"_\n\n## Value vs. pointer function arguments\n\nWhat if, instead of depending on the Writer interface, we called our own `write()` function?\n\n```go\nfunc main() {\n\tvar b strings.Builder\n\tfor i := 3; i >= 1; i-- {\n\t\twrite(b, fmt.Sprintf(\"%d...\", i))\n\t}\n\tb.WriteString(\"ignition\")\n\tfmt.Println(b.String())\n}\n\nfunc write(b strings.Builder, s string) {\n\tb.WriteString(s)\n}\n```\n\nRunning the code above in the [example sandbox](https://pkg.go.dev/strings#Builder) outputs just the word _ignition_. \n\nThe 3 calls to `write(b)` do not modify the builder declared at the top.\n\nThis makes sense, because passing a struct to a function [copies](https://tour.golang.org/methods/4) the struct value.\n\nTo fix this, we have to use a pointer to pass the struct by reference, and we have to invoke the function with `write(&b, ...)`. This works, but it doesn't make the code any more consistent.\n\n```go\nfunc main() {\n\tvar b strings.Builder\n\tfor i := 3; i >= 1; i-- {\n\t\twrite(&b, fmt.Sprintf(\"%d...\", i))\n\t}\n\tb.WriteString(\"ignition\")\n\tfmt.Println(b.String())\n}\n\nfunc write(b *strings.Builder, s string) {\n\tb.WriteString(s)\n}\n```\n\n## Why do the method calls work?\n\nWhy are we allowed to use `b` instead of `&b` in front of [b.WriteString](https://pkg.go.dev/strings#Builder.WriteString) and [b.String](https://pkg.go.dev/strings#Builder.String)? This is explained in [the tour](https://tour.golang.org/methods/6) as well.\n\n_\"...even though v is a value and not a pointer, the method with the pointer receiver is called automatically. That is, as a convenience, Go interprets the statement v.Scale(5) as (&v).Scale(5) since the Scale method has a pointer receiver.\"_\n\n## Start with a pointer\n\nIf all this mixing of values and pointers feels inconsistent, why not start with a pointer from the beginning?\n\nThe following code will compile just fine, but can you tell what's wrong with it?\n\n```go\nfunc main() {\n\tvar b *strings.Builder\n\tfor i := 3; i >= 1; i-- {\n\t\tfmt.Fprintf(b, \"%d...\", i)\n\t}\n\tb.WriteString(\"ignition\")\n\tfmt.Println(b.String())\n}\n```\nThe declaration above results in a nil pointer panic at run time, because b is uninitialized.\n\n> _panic: runtime error: invalid memory address or nil pointer dereference  \n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4991c7]_\n\n## Create the Builder with new()\n\nHere is one way to initialize a pointer so that it references a new Builder.\n\n```go\nfunc main() {\n\tb := new(strings.Builder)\n\tfor i := 3; i >= 1; i-- {\n\t\tfmt.Fprintf(b, \"%d...\", i)\n\t}\n\tb.WriteString(\"ignition\")\n\tfmt.Println(b.String())\n}\n```\n\n`new(strings.Builder)` returns a pointer to a freshly allocated Builder, which we can use for both functions and pointer receiver methods. This is the pattern which I now use in [shortscale-go](https://github.com/jldec/shortscale-go/blob/2485be23ef48660d8913b2ac884030220dc82d74/shortscale.go#L17-L24).\n\n\nAn alternative, which does the same thing, is the more explicit [struct literal](https://tour.golang.org/moretypes/5) shown below.\n\n```go\nfunc main() {\n\tb := &strings.Builder{}\n\t...\n```\n\n> There's no avoiding pointers in Go.  \n> Learn the quirks and the gotchas today.  \n> âœ¨ Keep learning! âœ¨\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/getting-started-with-go-part-2-pointers-4a47)_\n\n---- #excerpt ----\n\nPart 2 of my experience as a new user of Go."},{"path":"/getting-started-with-go-part-3-goroutines-and-channels.md","text":"---- /getting-started-with-go-part-3-goroutines-and-channels ----\ntitle: Getting started with Goroutines and channels\nimage: images/grape-hyacinth.jpg\ndate: 2021-04-25\ntemplate: post\nmetap-og;title: Getting started with Goroutines and channels\nmetap-og;image: https://jldec.me/images/grape-hyacinth.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/getting-started-with-go-part-3-goroutines-and-channels\nmetap-og;description: My experience as a new user of Goroutines and channels\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Getting started with Goroutines and channels\nmeta-twitter;description: My experience as a new user of Goroutines and channels\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/grape-hyacinth.jpg\nmeta-twitter;image;alt: Ladybug in Cambridge UK\n\n## Golang\n\nThis is part 3 of my experience as a new user of Go, focusing on concurrency with Goroutines and channels.\n\nFor installation, testing, and packages, see [Getting started with Go](/getting-started-with-go), and for pointers see [Getting started with Go pointers](/getting-started-with-go-part-2-pointers).\n\n## Counting HTTP requests\n\nThe [server](https://github.com/jldec/racey-go/blob/main/main.go) below counts HTTP requests, and returns the latest count on each request. \n\n_To follow along, clone https://github.com/jldec/racey-go, and start the server with 'go run .'_\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n)\n\nfunc main() {\n\tvar count uint64 = 0\n\n\thttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\tcount++\n\t\tfmt.Fprintln(w, count)\n\t})\n\n\tfmt.Println(\"Go listening on port 3000\")\n\thttp.ListenAndServe(\":3000\", nil)\n}\n```\n\n```sh\n$ curl localhost:3000\n1\n$ curl localhost:3000\n2\n```\n\nLet's try sending multiple requests at the same time. This command invokes curl with urls from a file using xargs to spawn 4 processes at once.\n\n```sh\n$ cat urls.txt | xargs -P 4 -n 1 curl\n```\n\nThe [file](https://github.com/jldec/racey-go/blob/main/urls.txt) contains 100 lines, but instead of ending on a nice round number, on systems with more than 1 core you may see  something like this (e.g. after 3 runs)\n\n```\n289\n292\n291\n```\n\nReplace the Go server with '[node server.js](https://github.com/jldec/racey-go/blob/main/server.js)' to compare the results (e.g. after 3 runs again)\n\n```\n298\n299\n300\n```\n\nNow repeat the experiment with the [race detector](https://golang.org/doc/articles/race_detector) turned on. The detector will report a problem on [line 12](https://github.com/jldec/racey-go/blob/main/main.go#L12) of main.go which is `count++`.\n\n```sh\n$ go run -race .\nGo listening on port 3000\n==================\nWARNING: DATA RACE\nRead at 0x00c000138280 by goroutine 7:\n  main.main.func1()\n      /Users/jleschner/pub/racey-go/main.go:12 +0x4a\n  net/http.HandlerFunc.ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2069 +0x51\n  net/http.(*ServeMux).ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2448 +0xaf\n  net/http.serverHandler.ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2887 +0xca\n  net/http.(*conn).serve()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:1952 +0x87d\n\nPrevious write at 0x00c000138280 by goroutine 9:\n  main.main.func1()\n      /Users/jleschner/pub/racey-go/main.go:12 +0x64\n  net/http.HandlerFunc.ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2069 +0x51\n  net/http.(*ServeMux).ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2448 +0xaf\n  net/http.serverHandler.ServeHTTP()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:2887 +0xca\n  net/http.(*conn).serve()\n      /Users/jleschner/go1.16.3/src/net/http/server.go:1952 +0x87d\n```\n\n## Data races\n\nFrom the [race detector](https://golang.org/doc/articles/race_detector) docs:\n\n_A data race occurs when two goroutines access the same variable concurrently and at least one of the accesses is a write._\n\n> It's clear that 'count++' modifies the count, but what are goroutines and where are they in this case?\n\n## Goroutines\n\nGoroutines provide low-overhead threading. They are easy to create and scale well on multi-core processors.\n\nThe Go runtime can schedule many concurrent goroutines across a small number of OS threads. Under the covers, this is how the [http](https://golang.org/src/net/http/server.go#L3013) library handles concurrent web requests.\n\nLet's start with an example. You can run it in the [Go Playground](https://play.golang.org/p/HdH4UQEEXuU).\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\tch := make(chan string)\n\n\t// start 2 countdowns in parallel goroutines\n\tgo countdown(\"crew-1\", ch)\n\tgo countdown(\"crew-2\", ch)\n\n\tfmt.Println(<-ch) // block waiting to receive 1st string\n\tfmt.Println(<-ch) // block waiting to receive 2nd string\n}\n\nfunc countdown(name string, ch chan<- string) {\n\tfor i := 10; i > 0; i-- {\n\t\tfmt.Println(name, i)\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\tch <- \"blastoff \" + name\n}\n```\n\nEach 'go countdown()' starts a new [goroutine](https://tour.golang.org/concurrency/1). Notice how the countdowns are interleaved in the output.\n\n```\n...\ncrew-1 3\ncrew-2 3\ncrew-2 2\ncrew-1 2\ncrew-1 1\ncrew-2 1\nblastoff crew-2\nblastoff crew-1\n```\n\n## Channels\n\n[Channels](https://tour.golang.org/concurrency/2) allow goroutines to communicate and coordinate.\n\nIn the example above, `<-ch` (receive) will block until another goroutine uses `ch <-` to send a string to the channel. This happens at the end of each countdown.\n\nSends will also block if there are no receivers, but that is not the case here.\n\nThere are many other variations for how to use channels, including [buffered channels](https://tour.golang.org/concurrency/3) which only block sends when the buffer is full.\n\n## Atomicity\n\nGiven that [net/http](https://pkg.go.dev/net/http) requests are handled by goroutines, can we explain why there is a data race when the function which handles a request increments a shared counter?\n\nThe reason is that `count++` requires a read followed by write, and these are not automatically synchronized. One goroutine may overwrite the increment of another, resulting in lost writes.\n\nTo fix this, the counter has be protected to make the increment operation atomic.\n\n## Counter-go\n\n[github.com/jldec/counter-go](https://github.com/jldec/counter-go) demonstrates 3 different implementations of a threadsafe global counter.\n\n1. **CounterAtomic** uses `atomic.AddUint64` and `atomic.LoadUint64`.\n2. **CounterMutex** uses `sync.RWMutex`.\n3. **CounterChannel** serializes all reads and writes inside 1 goroutine with 2 channels.\n\nAll 3 types implement a Counter interface:\n\n```go\ntype Counter interface {\n    Get() uint32 // get current counter value\n    Inc()        // increment by 1\n}\n```\n\nThe [modified server](https://github.com/jldec/racey-go/blob/fix-with-counter-go/main.go) will work with any of the 3 implementations, and no data race should be detected.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\n\tcounter \"github.com/jldec/counter-go\"\n)\n\nfunc main() {\n\tcount := new(counter.CounterAtomic)\n\t// count := new(counter.CounterMutex)\n\t// count := counter.NewCounterChannel()\n\n\thttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\tcount.Inc()\n\t\tfmt.Fprintln(w, count.Get())\n\t})\n\n\tfmt.Println(\"Go listening on port 3000\")\n\thttp.ListenAndServe(\":3000\", nil)\n}\n```\n\n### Coordination with channels\n\nOf the 3 implementations, [CounterChannel](https://github.com/jldec/counter-go/blob/main/counter_channel.go) is the most interesting. All access to the counter goes through 1 goroutine which uses a [select](https://tour.golang.org/concurrency/5) to wait for either a read or a write on one of two channels.\n\nCan you tell why neither `Inc()` nor `Get()` should block?\n\n```go\n\npackage counter\n\n// Thread-safe counter\n// Uses 2 Channels to coordinate reads and writes.\n// Must be initialized with NewCounterChannel().\ntype CounterChannel struct {\n\treadCh  chan uint64\n\twriteCh chan int\n}\n\n// NewCounterChannel() is required to initialize a Counter.\nfunc NewCounterChannel() *CounterChannel {\n\tc := &CounterChannel{\n\t\treadCh:  make(chan uint64),\n\t\twriteCh: make(chan int),\n\t}\n\n\t// The actual counter value lives inside this goroutine.\n\t// It can only be accessed for R/W via one of the channels.\n\tgo func() {\n\t\tvar count uint64 = 0\n\t\tfor {\n\t\t\tselect {\n\t\t\t// Reading from readCh is equivalent to reading count.\n\t\t\tcase c.readCh <- count:\n\t\t\t// Writing to the writeCh increments count.\n\t\t\tcase <-c.writeCh:\n\t\t\t\tcount++\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn c\n}\n\n// Increment counter by pushing an arbitrary int to the write channel.\nfunc (c *CounterChannel) Inc() {\n\tc.check()\n\tc.writeCh <- 1\n}\n\n// Get current counter value from the read channel.\nfunc (c *CounterChannel) Get() uint64 {\n\tc.check()\n\treturn <-c.readCh\n}\n\nfunc (c *CounterChannel) check() {\n\tif c.readCh == nil {\n\t\tpanic(\"Uninitialized Counter, requires NewCounterChannel()\")\n\t}\n}\n```\n\n### Benchmarks\n\nAll 3 [implementations](https://github.com/jldec/counter-go) are fast. Serializing everything through a goroutine with channels, costs only a few hundred ns for a single read or write. When constrained to a single OS thread, the cost of goroutines is even lower.\n\n```sh\n$ go test -bench .\ngoos: darwin\ngoarch: amd64\npkg: github.com/jldec/counter-go\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n```\n\n#### Simple: 1 op = 1 Inc() in same thread\n```sh\nBenchmarkCounter_1/Atomic-12                 195965660          6 ns/op\nBenchmarkCounter_1/Mutex-12                   54177086         22 ns/op\nBenchmarkCounter_1/Channel-12                  4499144        286 ns/op\n```\n\n#### Concurrent: 1 op = 1 Inc() across each of 10 goroutines\n```sh\nBenchmarkCounter_2/Atomic_no_reads-12          7298484        191 ns/op\nBenchmarkCounter_2/Mutex_no_reads-12           1966656        621 ns/op\nBenchmarkCounter_2/Channel_no_reads-12          256842       4771 ns/op\n```\n\n#### Concurrent: 1 op = [ 1 Inc() + 10 Get() ] across each of 10 goroutines\n```sh\nBenchmarkCounter_2/Atomic_10_reads-12          3922029        286 ns/op\nBenchmarkCounter_2/Mutex_10_reads-12            416354       2844 ns/op\nBenchmarkCounter_2/Channel_10_reads-12           21506      55733 ns/op\n```\n\n#### Constrained to single thread\n```sh\n$ GOMAXPROCS=1 go test -bench .\n\nBenchmarkCounter_1/Atomic                    197135869          6 ns/op\nBenchmarkCounter_1/Mutex                      55698454         22 ns/op\nBenchmarkCounter_1/Channel                     5689788        214 ns/op\n\nBenchmarkCounter_2/Atomic_no_reads            19519166         60 ns/op\nBenchmarkCounter_2/Mutex_no_reads              4702759        254 ns/op\nBenchmarkCounter_2/Channel_no_reads             530554       2197 ns/op\n\nBenchmarkCounter_2/Atomic_10_reads             6269979        189 ns/op\nBenchmarkCounter_2/Mutex_10_reads               927439       1354 ns/op\nBenchmarkCounter_2/Channel_10_reads              47889      25054 ns/op\n```\n\n\n> ðŸš€ - code safe - ðŸš€\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/getting-started-with-goroutines-and-channels-fc6)_\n\n---- #excerpt ----\n\nPart 3 in my learning Go series, focusing on concurrency with Goroutines and channels.\n"},{"path":"/getting-started-with-go.md","text":"---- /getting-started-with-go ----\ntitle: Getting started with Go\nimage: images/church-blossoms.jpg\ndate: 2021-04-03\ntemplate: post\nmetap-og;title: Getting started with Go\nmetap-og;image: https://jldec.me/images/church-blossoms.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/getting-started-with-go\nmetap-og;description: My experience as a new user of Go, building my first Go library\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Getting started with Go\nmeta-twitter;description: My experience as a new user of Go, building my first Go library\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/church-blossoms.jpg\nmeta-twitter;image;alt: Blossoms in Cambridge UK\n\n## Golang\n\nThe [Go](https://go.dev/) programming language has become an important tool for developers, particularly around platforms like Kubernetes and Docker.\n\nGo was created at Google by a team whose roots go back to Bell Labs and [C](https://en.wikipedia.org/wiki/C_(programming_language)). Their [motivations](https://talks.golang.org/2012/splash.article) included fast compilation time, and productive development of large scale distributed systems, handling high volumes of concurrent requests.\n\nThis article describes my experience as a new user of Go, building my first Go library. It follows a learning pattern similar to [forays from Node to Rust](/forays-from-node-to-rust).\n\n## Getting started\n\nThe [Tour of Go](https://tour.golang.org/basics/1) is a great way to get familiar with the language syntax. I started with 'hello world' at [golang.org](https://golang.org/doc/tutorial/getting-started#install), and found myself going back to the tour for different topics.\n\n[![Screenshot of the Go Tour showing code and navigation](/images/go-tour.png \".no-border\")](https://tour.golang.org/basics/1)\n\nThe macOS [installer](https://golang.org/doc/manage-install) copies everything into `/usr/local/go`, so I opted to download the latest release from https://golang.org/dl/ into a versioned [$GOROOT](https://golang.org/doc/install/source#environment) under my home directory. Here's what I have in my '.bash_profile':\n\n```sh\nexport GOPATH=~/go\nexport GOROOT=~/go1.16.3\nexport PATH=${PATH}:${GOROOT}/bin:${GOPATH}/bin\n```\n\n## VS Code\n\nThe [VS Code Go](https://github.com/golang/vscode-go/) extension has improved a lot over the years. It now auto-installs the [delve](https://github.com/go-delve/delve) debugger, and the [gopls](https://blog.golang.org/gopls-vscode-go) language server. I did not have to do any additional configuration.\n\nHovering over types like `Builder` shows source docs, and links to [pkg.go.dev](https://go.dev).\n\n![VS Code screenshot showing hover over strings.Builder](/images/go-vs-code.png)\n\n## Porting from Rust to Go\n\nI found it quite easy to port [shortscale-rs](https://github.com/jldec/shortscale-rs/blob/main/src/shortscale.rs#L15) to [shortscale-go](https://github.com/jldec/shortscale-go/blob/main/shortscale.go).\n\nGo has no ownership syntax, and the run-time includes a garbage collector.\n\nIn this case, I was also lucky that the Go [strings.Builder](https://pkg.go.dev/strings#Builder) standard library package is very similar to the [writer](https://github.com/jldec/shortscale-rs/blob/main/src/shortscale.rs#L46) pattern I ended up using for Rust.\n\n> Overall, I was pleasantly surprised with the readability of the code\n\n```go\npackage shortscale\n\nimport (\n\t\"strings\"\n)\n\n// Shortscale converts numbers into English words.\n// Supports positive integers from 0 to 999_999_999_999_999_999.\n// Larger values return \"(big number)\".\nfunc Shortscale(n uint64) string {\n\tif n <= 20 {\n\t\treturn numwords[n]\n\t}\n\tif n > 999_999_999_999_999_999 {\n\t\treturn \"(big number)\"\n\t}\n\tb := new(strings.Builder)\n\twriteScale(b, n, 1_000_000_000_000_000) // quadrillions\n\twriteScale(b, n, 1_000_000_000_000)     // trillions\n\twriteScale(b, n, 1_000_000_000)         // billions\n\twriteScale(b, n, 1_000_000)             // millions\n\twriteScale(b, n, 1_000)                 // thousands\n\twriteHundreds(b, n)\n\twriteTensAndUnits(b, n, b.Len() > 0)\n\treturn b.String()\n}\n\nfunc writeTensAndUnits(b *strings.Builder, n uint64, ifAnd bool) {\n\tn = n % 100\n\tif n == 0 {\n\t\treturn\n\t}\n\tif ifAnd {\n\t\twriteWord(b, \"and\")\n\t}\n\tif n <= 20 {\n\t\twriteWord(b, numwords[n])\n\t\treturn\n\t}\n\twriteWord(b, numwords[n/10*10]) // tens\n\tunits := n % 10\n\tif units > 0 {\n\t\twriteWord(b, numwords[units])\n\t}\n}\n\nfunc writeHundreds(b *strings.Builder, n uint64) {\n\tn = n / 100 % 10\n\tif n == 0 {\n\t\treturn\n\t}\n\twriteWord(b, numwords[n])\n\twriteWord(b, numwords[100])\n}\n\nfunc writeScale(b *strings.Builder, n uint64, thousands uint64) {\n\tn = n / thousands % 1_000\n\tif n == 0 {\n\t\treturn\n\t}\n\twriteHundreds(b, n)\n\twriteTensAndUnits(b, n, (n/100%10) > 0)\n\twriteWord(b, numwords[thousands])\n}\n\nfunc writeWord(b *strings.Builder, word string) {\n\tif b.Len() > 0 {\n\t\tb.WriteString(\" \")\n\t}\n\tb.WriteString(word)\n}\n\nvar numwords = map[uint64]string{\n\t0:                     \"zero\",\n\t1:                     \"one\",\n\t2:                     \"two\",\n\t3:                     \"three\",\n\t4:                     \"four\",\n\t5:                     \"five\",\n\t6:                     \"six\",\n\t7:                     \"seven\",\n\t8:                     \"eight\",\n\t9:                     \"nine\",\n\t10:                    \"ten\",\n\t11:                    \"eleven\",\n\t12:                    \"twelve\",\n\t13:                    \"thirteen\",\n\t14:                    \"fourteen\",\n\t15:                    \"fifteen\",\n\t16:                    \"sixteen\",\n\t17:                    \"seventeen\",\n\t18:                    \"eighteen\",\n\t19:                    \"nineteen\",\n\t20:                    \"twenty\",\n\t30:                    \"thirty\",\n\t40:                    \"forty\",\n\t50:                    \"fifty\",\n\t60:                    \"sixty\",\n\t70:                    \"seventy\",\n\t80:                    \"eighty\",\n\t90:                    \"ninety\",\n\t100:                   \"hundred\",\n\t1_000:                 \"thousand\",\n\t1_000_000:             \"million\",\n\t1_000_000_000:         \"billion\",\n\t1_000_000_000_000:     \"trillion\",\n\t1_000_000_000_000_000: \"quadrillion\",\n}\n```\n\n## Tests and benchmarks\n\nThe [testing](https://pkg.go.dev/testing) package provides support for running tests and benchmarks with `go test`. The GitHub Action [workflow](https://github.com/jldec/shortscale-go/blob/main/.github/workflows/ci.yaml#L25) for shortscale-go make use of this.\n\nOut of curiosity, I ran [BenchmarkShortscale](https://github.com/jldec/shortscale-go/blob/main/shortscale_test.go#L24) for two variants of the Shortscale function, one which [pre-allocates](https://github.com/jldec/shortscale-go/blob/358a49f24dcb9d4b2c697233f37f5dea4c87d318/shortscale.go#L18) memory for string.Builder, and one which does not. Pre-allocating, reduced the number of allocs/op from 4 to 1, improving ns/op by about 85ns.\n\n**Pre-allocated**\n```\n$ go test -bench . -benchmem\ngoos: darwin\ngoarch: amd64\npkg: github.com/jldec/shortscale-go\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n\n5694252\t       205.5 ns/op\t      64 B/op\t       1 allocs/op\n```\n\n**Not pre-allocated**\n```\n$ go test -bench . -benchmem\ngoos: darwin\ngoarch: amd64\npkg: github.com/jldec/shortscale-go\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n\n4100697\t       292.9 ns/op\t     120 B/op\t       4 allocs/op\n```\n\n\n## Dependency management\n\nUntil quite recently, Go [did not have](https://research.swtch.com/vgo-intro#versioning_and_api_stability) built-in package versioning like [npm](/migrating-from-cjs-to-esm) or [cargo](/forays-from-node-to-rust). This led to incompatibile versioning add-ons, like [godep](https://github.com/tools/godep) and [glide](https://github.com/Masterminds/glide), which made packages with nested dependencies difficult to consume. E.g. see this old [INSTALL.md](https://github.com/kubernetes/client-go/blob/416948da08dfd61cd4a08a3d679865ce91ff39b6/INSTALL.md#dependency-management-for-the-serious-or-reluctant-user) from kubernetes/client-go.\n\n> Fortunately, [Go modules](https://blog.golang.org/using-go-modules) are enabled as the default in Go since [v1.16](https://blog.golang.org/go116-module-changes).\n\n## go.mod\n\nI created my [shortscale-go](https://github.com/jldec/shortscale-go/blob/main/go.mod) module with [go mod init](https://golang.org/pkg/cmd/go/#hdr-Module_maintenance) following the guide in [Using Go Modules](https://blog.golang.org/using-go-modules).\n\n```sh\n$ go mod init github.com/jldec/shortscale-go\ngo: creating new go.mod: module github.com/jldec/shortscale-go\n```\nThis created a new **go.mod** file with the following content.\n\n```js\nmodule github.com/jldec/shortscale-go\n\ngo 1.16\n```\n\nI was a little surprised that there was no way to indicate the module version inside `go.mod`. Go [relies on git tags](https://blog.golang.org/publishing-go-modules) in the form vx.x.x for this. As I pushed each version to GitHub, I used the GitHub [Releases](https://github.com/jldec/shortscale-go/releases) UI to create the tag.\n\n## pkg.go.dev\n\nThe shortscale package is published at https://pkg.go.dev/github.com/jldec/shortscale-go\n\n[![Module page for shortscale-go on go.dev](/images/shortscale-go-go-dev.png)](https://pkg.go.dev/github.com/jldec/shortscale-go)\n\nIt turns out that fetching any versioned module with `go get`, automatically adds that module to the registry at [go.dev](https://go.dev/about). This feels a little strange at first, but the more I use it, the more I think it's a clever solution.\n\n> How about using a similar scheme to create a vendor-neutral registry for [ESM modules](/extracting-an-esm-module-from-a-deno-script)?  \nðŸ¤”\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/getting-started-with-go-2m9e)_\n\n---- #excerpt ----\n\nThe [Go](https://go.dev/) programming language has become an important tool for developers, particularly around platforms like Kubernetes and Docker.\n\n"},{"path":"/github-actions-101.md","text":"---- /github-actions-101 ----\ntitle: GitHub Actions 101\nimage: images/snowy-boathouse.jpg\ndate: 2020-11-27\ntemplate: post\n\n# How do GitHub Actions work?\n\nToday I was feeling a bit lost (again, sigh) trying to understand GitHub Actions.\n\nSpecifically, the [documentation](https://docs.github.com/en/free-pro-team@latest/actions) did not appear to have an overview of how actions are composed and what they are composed of. What are those things that run? How are they named and referenced?\n\nIn retrospect, instead of the docs, I would recommend getting started by looking at the yaml for the [Simple workflow](https://github.com/actions/starter-workflows/blob/main/ci/blank.yml). The button appears in the __Actions__ tab on any new repo.\n\n![Screenshot of GitHub Actions tab on new repo, showing sample workflow](/images/actions-start-here.png)\n\nThe button will open the workflow yaml in an editor (it won't be committed to your repo yet).\n\n![Screenshot of Simple workflow yaml code](/images/simple-workflow.png)\n\n## Not so mysterious after all _!smile-o_\n\n> The first takeaway is that actions can be written using simple shell commands.\n\nCommands run in a shell in VMs which come preinstalled with [commonly used tools](https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu1804-README.md). There is no need to learn a new scripting language. You can even write your action as a shell script in a file, and invoke it from the workflow yaml.\n\nIf you want, you can package and re-use portions of a job in different workflows. These components are also called [actions](https://docs.github.com/en/actions/creating-actions) (that's where some of my initial confusion originated), _but you don't need to learn to write those yourself in order to build your own workflows_.\n\n## Don't be thrown off by the yaml\n\nEach job is identified by its key e.g. `simple-job` in the example below.  \nThe [workflow syntax](https://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-syntax-for-github-actions) documentation denotes this as `jobs.<job-id>`.\n```yaml\non: push\njobs:\n  simple-job:\n    runs-on: ubuntu-latest\n    env:\n      HELLO: world\n    steps:\n    - run: 'echo Hello: $HELLO'\n    - run: |\n        echo node version: `node -v`\n        pwd\n```\n\n`steps:` contains a list of commands, each described by a `run:`  \n(In the earlier example above, there is also an [action](https://docs.github.com/en/free-pro-team@latest/actions/creating-actions/about-actions), described with `uses:` instead of `run:`)\n\nThe 1st run: command above is quoted, to avoid \": \" being interpreted as a yaml map.\n\nThe 2nd run: contains 2 commands in a multi-line (indented) block using \"|\". This syntax does not require quotes, making it convenient for embedded scripts.\n\nIf you push the workflow above to a new repo on GitHub, the result should look like this:\n\n![Screenshot of workflow run showing simple-job output](/images/action1.png)\n\n### A few more things to know about [yaml](https://en.wikipedia.org/wiki/YAML)\n\n- You can test the validity of your yaml [online](https://onlineyamltools.com/convert-yaml-to-json).  \n  Seeing the JSON representation will often clear up confusion.\n- Quotes around (scalar) strings are optional, BUT there are a lot of gotchas!  \n  E.g. look out for strings with '- ' at the start, or ': ' or '#' in the middle, or ':' at the end, or strings that look like numbers or booleans.\n- Maps are usually written as `key: value` lines at the same indentation.  \n  Lists are usually written as lines with `- value`.  \n  _Or_ use `{key1: v1, key2: v2, ...}` for maps, `[v1, v2, ...]` for lists.\n- Indentation must use spaces (no tabs).\n- For more about multi-line strings see https://yaml-multiline.info/.\n\n\n> Have fun playing with GitHub Actions  \n> _!gears_\n\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/github-actions-101-pfn)_\n\n---- #excerpt ----\n\nNot so mysterious after all _!smile-o spin_"},{"path":"/migrating-from-cjs-to-esm.md","text":"---- /migrating-from-cjs-to-esm ----\ntitle: Migrating from CommonJS to ESM\nimage: images/calm.jpg\ndate: 2021-01-23\ntemplate: post\n\n## Node and npm modules\n\n[Node.js](https://nodejs.org/en/docs/guides/getting-started-guide/) opened the door for developers to build performant web servers using JavaScript.\n\nThe explosion of [CommonJS](https://nodejs.org/docs/latest/api/modules.html#modules_modules_commonjs_modules) modules which followed, created a massive new ecosystem. Building a typical website today involves hundreds, if not thousands, of modules.\n\nTo publish a module, you set `module.exports` in your code, create a `package.json` file, and run `npm publish`.\n\nTo consume a module, you add a dependency to your `package.json` file, run `npm install`, and call `require('module-name')` from your code.\n\nModules can depend on other modules.\n\n[Npm](https://docs.npmjs.com/about-npm) moves module files between a central registry and the machines running Node.js.\n\n## ESM modules\n\nIn [2015](https://262.ecma-international.org/6.0/#sec-ecmascript-language-scripts-and-modules), `import` and `export` statements were added to JavaScript. ESM module loading is now a built-in feature of [all major browsers](https://caniuse.com/mdn-javascript_statements_import) (sorry IE.)\n\nESM removes the need for package.json files, and uses URLs instead of npm module names -- but it does not preclude those from being used with ESM, say in a Node.js context.\n\nTo publish an ESM module, use `export` in your code, and make the file fetchable by URL.\n\nTo consume an ESM module, use `import { ... } from URL`. See [MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import) for more details.\n\nUsing `import` instead of `require()` allows ESM modules to be loaded independently, without running the code where they are used. A variant of the `import` statement, is the [dynamic import()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import#dynamic_imports) function. This allows for modules to be loaded asynchronously at run-time.\n\n> ESM is the basis for exciting new developer tools like [Snowpack](https://github.com/snowpackjs/snowpack#readme) and [Vite](https://github.com/vitejs/vite#readme).\n\n## So, why are most modules still published with CommonJS?\n\nEven before ESM, developers could use npm modules in front-end code.  Tools like [browserify](https://github.com/browserify/browserify#readme) or [webpack](https://github.com/webpack/webpack#readme) bundle modules into a single script file, loadable by browsers.\n\nOn the server side, it has taken Node.js a few years to arrive at [ESM support](https://nodejs.org/api/packages.html#packages_determining_module_system). Unfortunately, the 2 standards are not fully interoperable.\n\nDespite everyone's best intentions, the [Node.js docs](https://nodejs.org/api/esm.html#esm_interoperability_with_commonjs) are unclear about what to do. For a deeper explanation, I recommend [this article](https://redfin.engineering/node-modules-at-war-why-commonjs-and-es-modules-cant-get-along-9617135eeca1) by Dan Fabulich.\n\nHere is a summary of some interop scenarios:\n\n#### require() from default Node.js context\n- require(\"CommonJS-module\") - **Yes _!check_**, this has always worked and is the default.\n- require(\"ESM-module\") - **No _!close_**.\n- require(\"Dual-ESM-CJS-module\") - **Yes _!check_**, but be careful with state.\n\n#### import statement from Node.js ESM context - E.g. in a server.mjs file.\n- import from \"ESM-module\" - **Yes _!check_**.\n- import default from \"CommonJS-module\" - **Yes _!check_**.\n- import { name } from \"CommonJS-module\" - **No _!close_**, get default.name from 2.\n\n## Dynamic Import as a fallback\nNode's inability to require() ESM modules prevents simple upgrades from CommonJS to ESM.\n\nPublishing [dual](https://nodejs.org/dist/latest-v15.x/docs/api/packages.html#packages_dual_commonjs_es_module_packages) ESM-CJS packages is messy because it involves [wrapping](https://redfin.engineering/node-modules-at-war-why-commonjs-and-es-modules-cant-get-along-9617135eeca1#6b50) CommonJS modules in ESM. Writing a module using ESM and then wrapping it for CommonJS is not possible.\n\nFortunately, [dynamic import()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import#dynamic_imports) provides an alternative.\n\nDynamic import() works from the default Node.js context as well as from an ESM context. You can even import() CJS modules. The only gotcha is that it returns a promise, so it is not a drop-in replacement for require().\n\nHere is an example showing require() and import() together.\n\nI published [shortscale](https://github.com/jldec/shortscale) v1 as CommonJS. For [v2 and later](https://github.com/jldec/shortscale/pull/2) the module is only available as ESM. This means that later releases can no longer be loaded using Node.js require().\n\nThis [fastify server](https://github.com/jldec/demo-fastify-esm) loads both module versions from a CJS context.\n\n```js\n// minimal fastify server based on:\n// https://www.fastify.io/docs/latest/Getting-Started/#your-first-server\n\nconst fastify = require('fastify')({ logger: true });\n\nfastify.register(async (fastify) => {\n  let shortscale_v1 = require('shortscale-v1');\n  let shortscale_v4 = (await import('shortscale-v4')).default;\n\n  // e.g. http://localhost:3000/shortscale-v1?n=47\n  fastify.get('/shortscale-v1', function (req, res) {\n    let num = Number(req.query.n);\n    let str = '' + shortscale_v1(num);\n    res.send({num, str});\n  });\n\n  // e.g. http://localhost:3000/shortscale-v4?n=47\n  fastify.get('/shortscale-v4', function (req, res) {\n    let num = Number(req.query.n);\n    let str = '' + shortscale_v4(num);\n    res.send({num, str});\n  });\n});\n\n// Run the server!\nfastify.listen(3000, function (err, address) {\n  if (err) {\n    fastify.log.error(err);\n    process.exit(1);\n  }\n  fastify.log.info(`server listening on ${address}`);\n});\n```\n\nFor this demo, `package.json` installs both versions of shortscale.\n\n```json\n{\n  \"name\": \"demo-fastify-esm\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Demonstrate ESM dynamic import from non-ESM server\",\n  \"main\": \"server.js\",\n  \"scripts\": {\n    \"start\": \"node server.js\"\n  },\n  \"author\": \"Jurgen Leschner\",\n  \"license\": \"MIT\",\n  \"dependencies\": {\n    \"fastify\": \"^3.11.0\",\n    \"shortscale-v1\": \"npm:shortscale@^1.1.0\",\n    \"shortscale-v4\": \"npm:shortscale@^4.0.0\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/jldec/demo-fastify-esm\"\n  }\n}\n```\n\n> I plan to migrate my modules to ESM. Other [module authors](https://blog.sindresorhus.com/get-ready-for-esm-aa53530b3f77) are too.\n\n> _!cubes 3x_\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/migrating-from-commonjs-to-esm-2p24)_\n\n---- #excerpt ----\n\nHow to migrate from CommonJS to EcmaScript Modules.\n\n"},{"path":"/preventing-concurrent-github-actions.md","text":"---- /preventing-concurrent-github-actions ----\nalias: /serializing-github-actions-with-workers-durable-objects\ntitle: Preventing concurrent GitHub Actions\nimage: images/rose.jpg\ndate: 2021-06-13\ntemplate: post\nmetap-og;title: Preventing concurrent GitHub Actions\nmetap-og;image: https://jldec.me/images/rose.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/getting-started-with-go\nmetap-og;description: Use the new concurrency group feature to prevent concurrent GitHub Actions workflow runs\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Preventing concurrent GitHub Actions\nmeta-twitter;description: Use the new concurrency group feature to prevent concurrent GitHub Actions workflow runs\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/rose.jpg\nmeta-twitter;image;alt: Grape Hyacinths in Cambridge UK\n\n## GitHub Actions Workflows\n\nWhat happens when you trigger a [GitHub Actions](/github-actions-101) workflow which is already running? Workflows which depend on being run one-at-a-time might fail.\n\nI recently encountered this with a [workflow](https://github.com/jldec/cloudflare-pages-test/blob/main/.github/workflows/generate.yaml) for publishing a static website. This workflow generates HTML files which are pushed to another git repo for publishing by [GitHub Pages](https://pages.github.com/).\n\n![Screenshot of Github Actions log showing failed git push](/images/fail-generate.png)\n\nWhen two workflows try to push to a checked-out repo at the same time, one will fail because it is missing the last commit from the other.\n\n This is just one example where concurrent workflows are problematic. Workflows which [automate](https://github.community/t/serializing-workflow-runs-in-the-context-of-continuous-deployment/17559) [deployments](https://github.community/t/how-to-limit-concurrent-workflow-runs/16844) have the same [problem](https://github.community/t/serializing-queueing-deployment-workflows-aws-re-invent/17152).\n\nA number of 3rd party [solutions](https://github.com/softprops/turnstyle) exist, but these introduce additional waiting costs or other issues. For one of my projects, I host a lock-service, just to force concurrent workflows to exit quietly, and then auto-trigger re-runs.\n\n> Finally, on April 19, 2021,  \n[this](https://github.blog/changelog/2021-04-19-github-actions-limit-workflow-run-or-job-concurrency/) appeared in the GitHub Blog.\n\n![Screenshot of GitHub Blog from April 19, 2021 announcing the new concurrency key in GitHub Actions](/images/github-actions-concurrency-announcement.png)\n\nIn the case of using actions to generate a GitHub Pages website, [the feature works](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#concurrency) exactly as required.\n\n- The first workflow will run to completion.\n- Subsequent concurrent workflows will either be delayed or cancelled.\n- In the end only the first and last of the overlapping workflows will be run.\n\nAnd all you need is [2 lines of yaml](https://github.com/jldec/cloudflare-pages-test/blob/main/.github/workflows/generate.yaml#L5-L6).\nThis is from the workflow which generates [jldec.uk](https://jldec.me/first-steps-using-cloudflare-pages#github-pages).\n\n[![Screenshot of yaml for GitHub Action with concurrency group](/images/github-actions-concurrency-yaml.png)](https://github.com/jldec/cloudflare-pages-test/blob/main/.github/workflows/generate.yaml#L5-L6)\n\nThe `group` can be any string - workflows in the same group are effectively serialized.\n\n> Thank you GitHub!\n\n---- #excerpt ----\n\nLearn how the new concurrency group feature for GitHub Actions prevents concurrent workflows.\n"},{"path":"/running-a-compiled-deno-script-in-a-github-action.md","text":"---- /running-a-compiled-deno-script-in-a-github-action ----\ntitle: Running a compiled Deno script in a GitHub Action\nimage: images/first-blossoms-2021.jpg\ndate: 2021-03-14\ntemplate: post\n\n## TL;DR\n\nThis is a quick followup to my recent post about [Getting Started with Deno](/getting-started-with-deno).\n\nIn this post I will enhance a GitHub Action to do the following:\n\n- Generate HTML using a static site generator (SSG).\n- Launch a preview web server which runs in the background.\n- Download a compiled Deno script.\n- Invoke the Deno script to scan for broken links in the generated HTML.\n- Only publish the HTML if there are no broken links.\n\n## scan.js\n\nI cross-compiled [scan.js](https://github.com/jldec/deno-hello/blob/main/scan.js) using Deno v1.8.1, and uploaded the resulting binaries to a [release](https://github.com/jldec/deno-hello/releases) on GitHub.\n\nUsing the releases feature of GitHub is a convenient way to publish compiled artifacts. In this case I used the manual upload feature in GitHub, but this step could be automated as well.\n\n![Releases with compiled artifacts in Repo deno-hello](/images/deno-scan-releases.png)\n\nHere is the deno compile [command](https://github.com/jldec/deno-hello/blob/main/compile.sh#L1). The binary for Linux is called `scan-linux-x86`.\n\n```\ndeno --unstable compile \\\n  --allow-net \\\n  --lite \\\n  --target x86_64-unknown-linux-gnu \\\n  --output scan-linux-x86 \\\n  scan.js\n```\n\nThe resulting binaries are quite large (~50MB), even using [--lite](https://deno.land/manual@v1.7.4/tools/compiler#generating-smaller-binaries), but I expect that to improve.\n\n## .github/workflow/generate.yaml\n\nIn this case, I modified the [workflow](https://github.com/jldec/cloudflare-pages-test/blob/main/.github/workflows/generate.yaml) for the static site in [jldec/cloudflare-pages-test](https://github.com/jldec/cloudflare-pages-test). Here is the excerpt of the yaml for the relevant step.\n\n```yaml\n- name: generate output\n  run: |\n    ...\n    npm run generate\n    npm run preview &\n    curl -LO https://github.com/jldec/deno-hello/releases/download/v1.0.2/scan-linux-x86 && chmod +x scan-linux-x86\n    ./scan-linux-x86 http://localhost:3001/\n    ...\n```\n\n`npm run generate` invokes the static site generator.  \n\n`npm run preview &` starts the preview server on port 3001, running in the background.\n\nBoth commands are defined as scripts in [package.json](https://github.com/jldec/cloudflare-pages-test/blob/main/package.json).\n\n`curl -LO` downloads the Linux binary from the GitHub release. Running curl here gives the preview server time to start listening on port 3001, before commencing the scan.\n\n## Success\n\nWhen the `scan-linux-x86` command finds no broken links in the static site, it exits with 0, allowing the GitHub Action [workflow](https://github.com/jldec/cloudflare-pages-test/runs/2112253519?check_suite_focus=true#step:4:72) to continue.\n\n![scan success in GitHub Action log output](/images/scan-success.png)\n\nIf there are broken links the workflow will [fail](https://github.com/jldec/cloudflare-pages-test/runs/2106962300?check_suite_focus=true), and I will hear about it in my inbox :)\n\n![scan failure in GitHub Action log output](/images/scan-failure.png)\n\n> [![Deno logo](/images/deno-logo.png \".no-border\")](https://deno.land/)\n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/running-a-compiled-deno-script-in-a-github-action-4ljn)_\n\n---- #excerpt ----\n\nThis is a followup to [Getting Started with Deno](/getting-started-with-deno).\nIn this post I enhance a GitHub Action to invoke the compiled scan.js Deno script which scans for broken links in generated HTML pages.\n\n"},{"path":"/spring-boot-101.md","text":"---- /spring-boot-101 ----\ntitle: Spring Boot 101\nimage: images/boot1.jpg\ndate: 2017-06-28\ntemplate: post\n\n# What is a Spring Bean really?\n\nToday I was feeling rather lost in my attempts to understand what is going on inside a Spring Boot application.\n\nBeing new to Spring, I had spent a fair bit of time googling and reading, but most of the things i came across simply assume prior knowledge when it comes to fundamental concepts like 'Beans'.\n\nMark Fisher, my friend and colleague here on the Spring team at Pivotal, came to my rescue and offered to explain. In exchange, I promised to write about it.\n\nThe result is this blog post and https://github.com/markfisher/spring-boot-hello-world.\n\n## In the beginning there was XML\n\nSpring started by providing a much-simpler-than-J2EE, XML-based way to configure the main classes that your Java application depended on.\n\nSimply instantiating everything with `new` in Java was not ideal because this meant that you had to hardwire all those dependencies into your code, so you couldn't easily use say one database for local testing and a different database for CI or production.  \n\nInstead of writing your own factories which abstracted the creation of things like database connections, you could use Spring and provide the details of all those things in XML.\n\nSpring would read some environment-specific XML at startup and create instances of those classes based on what was in the XML.\n\nAnd those instances were... _**Beans**_, something like this:\n\n![ApplicationContext.getBean()](/images/boot1b.jpg)\n\n## Now there are @Annotations\n\nFast forward a few years and the XML has been replaced by Java code annotated as `@Configuration` and `@ConfigurationProperties`.\n\n```java\n@Configuration\n@EnableConfigurationProperties(GreeterProperties.class)\npublic class GreeterAutoConfiguration {\n\n\t@Bean\n\tpublic Greeter greeter(GreeterProperties properties) {\n\t\treturn new Greeter(properties.getGreeting());\n\t}\n}\n```\n\n\nNotice that the factory method is annotated as `@Bean`.\n\nIf the configuration class is included in the `META-INF/spring.factories` of a dependent jar, then a Spring Boot application will automatically call the factory method to create a singleton instance of that class on startup.\n\n> This is the magic  \n> and those instances are still called **Beans**\n\n## What about [Spring Initializr](https://start.spring.io/) and all those starters?\n\nNow that [_configuration scanning_](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#using-boot-configuration-classes) is no longer such a mystery, the role of Spring Boot Starters is easier to understand.\n\nBy including a starter in the dependencies of your Boot App, you are telling Spring to scan for configuration classes inside that starter, which results in the automatic creation of the **Beans** for that starter.\n\n![Initializr injection](/images/boot7.jpg)\n\n> Easy!  \n> _!lemon-o_\n\n---\n\n\n---- #excerpt ----\n\nSpring Beans are magic - but what's behind them and why are they called beans?\n"},{"path":"/using-gitpod-to-create-a-pr.md","text":"---- /using-gitpod-to-create-a-pr ----\ntitle: Using Gitpod to create a PR\nimage: images/fields-of-clouds.jpg\ndate: 2021-10-24\ntemplate: post\nmetap-og;title: Using Gitpod to create a PR\nmetap-og;image: https://jldec.me/images/fields-of-clouds.jpg\nmetap-og;type: article\nmetap-og;url: https://jldec.me/using-gitpod-to-create-a-pr\nmetap-og;description: Gitpod hosts workspaces for developers. Each workspace is a Linux container running in the cloud, with a fully functional development environment, and an instance of VS Code which you can open in your browser.\nmeta-twitter;site: @jldec\nmeta-twitter;creator: @jldec\nmeta-twitter;title: Using Gitpod to create a PR\nmeta-twitter;description: Gitpod hosts workspaces for developers. Each workspace is a Linux container running in the cloud, with a fully functional development environment, and an instance of VS Code which you can open in your browser.\nmeta-twitter;card: summary_large_image\nmeta-twitter;widgets;new-embed-design: on\nmeta-twitter;image: https://jldec.me/images/fields-of-clouds.jpg\nmeta-twitter;image;alt: fields-of-clouds in Cambridge UK\n\n\n## Gitpod Workspaces\n\n[Gitpod](https://www.gitpod.io/) hosts **workspaces** for developers.\n\nThink of each [workspace](https://www.gitpod.io/docs#your-computer-in-the-cloud) as your own Linux container in the cloud, with a fully functional development environment, including:\n\n- A clone of your git repo and your git working tree - the files you're working on.\n- The tools you need while coding - compilers, SDKs, runtimes.\n- Your editor - the default is [VS Code](https://www.gitpod.io/blog/openvscode-server-launch) + extensions - reachable through a browser.\n- Shell access to run commands in the container.\n\nInstead of laboriously maintaining your local development environment with everything you need, you simply open a new workspace every time you start working on a new project or branch.\n\n## PR for the Gitpod website \n\nHere's how I created a PR for the [Gitpod website](https://www.gitpod.io/) in just a few minutes.\n\nStarting from the [issue](https://github.com/gitpod-io/website/issues/1139), I opened a new workspace in Gitpod by prefixing the GitHub url like so: `https://gitpod.io/#https://github.com/gitpod-io/website/issues/1139`. \n\nThat [url](https://gitpod.io/#https://github.com/gitpod-io/website/issues/1139) opened the workspace in my browser.\n\n![Screenshot of full VS Code window in Gitpod workspace](/images/gitpod-workspace.png)\n\nRunning `git branch -vv` in a workspace terminal showed that git was already set to a new branch, conveniently named with my username and the description and id of the GitHub issue.\n\n![Screenshot of VS Code terminal in Gitpod workspace showing new git branch](/images/gitpod-issue-branch.png)\n\nThe workspace started up with npm modules already installed by a [prebuild](https://www.gitpod.io/docs/prebuilds).\n\nThe repo was also [configured](https://www.gitpod.io/docs/config-gitpod-file) to start a dev server listening on port 3000 in the workspace container. The open ports can be seen in the Remote Explorer sidebar on the left.\n\n![Screenshot of VS Code Remote Explorer sidebar in Gitpod workspace showing open ports](/images/gitpod-ports.png)\n\nI used the `Open Browser` icon to open the website in another browser window, and watched my changes taking effect each time I modified the code. \n\nFinally, I pushed a commit with my changes on the new branch to GitHub, and proceeded to create my PR as usual. No localhost interaction other than running my browser was required.\n\n> This blogpost was written from a Gitpod workspace, and I recently joined the awesome team at Gitpod myself.  \n> ðŸš€ \n\n_To leave a comment  \nplease visit [dev.to/jldec](https://dev.to/jldec/using-gitpod-to-create-a-pr-3cba)_\n\n---- #excerpt ----\n\nGitpod hosts workspaces for developers. Each workspace is a Linux container running in the cloud, with a fully functional development environment, and an instance of VS Code which you can open in your browser."},{"path":"/why-serverless-at-the-edge.md","text":"---- /why-serverless-at-the-edge ----\ntitle: Why Serverless at the Edge?\nimage: /images/wires.jpg\ndate: 2019-07-11\ntemplate: post\n\n## Why tho?\n\nOne of the more memorable moments from today's [#ServerlessDays](https://twitter.com/ServerlessLDN) event in London was [@monkchips](https://twitter.com/monkchips)' rather emphatic pronouncement of skepticism about Serverless at the edge.\n\nIt happened right before the Keynote which was all about WebAssembly, and for which running at the edge is an important use case.\n\nSeveral speakers went on to make a strong case for technologies like WASM and WASI. \n\nThis one by Cloudflare's own [@nodebotanist](https://twitter.com/nodebotanist), was the most profound.  \n\n![@nodebotanist quote: \"The future of serverless lies in doing things in different ways. Creating new serverless architectures from the ground up and allowing users to try different strategies is a key part of any movement's maturity.\"](/images/kas-quote-2.jpg)\n\n> But what makes Serverless at the edge useful in ways that more-centralized services are not?\n\n## The edge is a trust boundary\n\nI think it's helpful to understand the edge as the boundary where the wild Internet first touches something with a DNS name and an SSL certificate. \n\nThis boundary is where, as a first order of business, we have a chance to protect Web properties from intruders and DDOS attacks ... a capability which remains Cloudflare's raison d'Ãªtre today.\n\n> Shouldn't this trust boundary be available to everyone?\n\nProviding public websites with protection is a valuable service, but what if we could do something similar (at the edge) to provide regular people with the means to communicate and collaborate in private.\n\nSharing photos with friends and relatives should be as simple as choosing a DNS name, and installing software that you trust at the edge where your DNS name meets the Internet. \n\nThis software will give you control over who gets to see your photos, and your relatives will be able to visit without surveillance or ads.\n\nPhotos are just one example. Sharing documents, sharing code, sharing work ... communicating privately is fundamental to our lives and to our economy.\n\n> Serverless at the edge has the potential to make the Web great again, for all of us.  \n>\n> _!globe 2x_\n\n---- #excerpt ----\n\nWhat makes Serverless at the edge useful in ways that more-centralized cloud services are not?\n"},{"path":"/why-the-web-needs-better-html-editing-components.md","text":"---- /why-the-web-needs-better-html-editing-components ----\ntitle: Why the Web needs better HTML editing components\nimage: /images/bridge.jpg\ndate: 2015-08-13\ntemplate: post\n\nYesterday I made a small contribution to help [**ProseMirror**](https://www.indiegogo.com/projects/prosemirror/#/story). Here's why.\n\n## HTML is awesome\n\nHTML has evolved into the de-facto rendering language for displays, and it just keeps getting better. Fonts, animations, sophisticated text layout, international character sets, vector images, high-resolution photos and video, the list goes on and on.\n\nThere's little doubt in 2015 that HTML has become **the** way to publish online.\n\n\n## People are still writing with tools designed for print\n\nThere must be good reasons why people are still using the old desktop editors, they're not exactly fun or easy to use.\n\nSome of it has to do with inertia, and proprietary formats, and interoperability. How else can you work together on a document if you can't share and open each other's files?\n\n\n## But let's be honest\n\nHTML, and it's siblings like CSS, with all their power and sophistication, have become so complex, that they are impossible to edit by anyone except experts. So, what's the answer?\n\n> How do people write and publish content in HTML?\n\n\n## Silos are not the solution\n\nHiding the complexity of creating and publishing HTML online is a _necessary_ service: But let's not fall into the trap of assuming that [website-hosting providers](https://www.squarespace.com/) and [commercial online publishers](https://medium.com/) are the only legitimate providers of this service. David Winer has [written](http://myword.io/users/davewiner/essays/051.html) about this.\n\n\n## We need *simple* personal publishing tools\n\n1. Write your story\n2. Illustrate it with a photo or two\n3. Publish it in beautifully designed HTML for others to enjoy\n\nClearly there are some missing pieces. One of those is **easier private web hosting**, but that's a topic for another blog post.\n\n> How do people write in HTML?\n\n\n## It's the Software, stupid\n\nWe know it takes experts to design beautiful HTML. So we need software to help the rest of us pour our stories into those HTML designs. It's a bit like sending a document to the printer, except that software can do it instantly, generating HTML for us, every keystroke we type.\n\nThis is where ProseMirror comes in. It will make editing HTML *feel* like [wysiwyg](https://en.wikipedia.org/wiki/WYSIWYG) even though the HTML we're editing is not our own. HTML will come from templates carefully crafted by  designers.\n\nEditing with these HTML templates will produce two separate things:\n\n1. HTML output, ready for publishing\n2. The stuff we write\n\n\n## And where do we store the stuff we write?\n\nI think the software community has the best answer to this question. Most of the code which powers our lives, lives in human-readable text files. These, in turn, live in services like [GitHub](https://github.com/jldec/) where we can track changes and collaborate on content together.\n\nLet's build [tools to do the same](https://jldec.github.io/pub-doc/how-it-works) for the stuff we write.\n\n\n> Thank You\n\n---\n\n---- #excerpt ----\n\nHTML, and it's siblings like CSS, with all their power and sophistication, have become so complex, that they are impossible to edit by anyone except experts. So, what's the answer?\n\nHow do people write and publish content in HTML?\n"}]},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog/templates","_pkg":"pub-theme-pubblog","name":"templates","type":"FILE","watch":false,"files":[{"path":"/default.hbs","text":"{{{html}}}\n"},{"path":"/doc-layout.hbs","text":"<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\n\n{{#eachMetaProp~}}\n<meta property=\"{{property}}\" content=\"{{content}}\">\n{{/eachMetaProp~}}\n{{#eachMetaSemi~}}\n<meta name=\"{{name}}\" content=\"{{content}}\">\n{{/eachMetaSemi~}}\n{{{metaSeo}}}\n\n<!-- html generated by pub-server from markdown {{_file.path}} -->\n\n<title>{{title}}</title>\n{{{injectCss}}}\n<!--[if lt IE 9]>\n<script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"></script>\n<script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n<![endif]-->\n</head>\n<body>\n\n{{{renderLayout}}}\n\n{{{injectJs}}}\n{{{comment \"copyright\"}}}\n</body>\n</html>\n"},{"path":"/home.hbs","text":"{{#eachPost~}}\n<div class='postentry'>\n{{{permaLink}}}\n<h2>{{{pageLink}}}</h2>\n<div class=\"date\">{{date}}</div>\n{{{fragmentHtml [#excerpt]}}}\n</div>\n{{/eachPost}}\n"},{"path":"/main-layout.hbs","text":"<header>\n<div id=\"doctitle\">{{{docTitle}}}</div>\n<div id=\"topmenu\">{{{topMenu}}}</div>\n</header>\n\n<div id=\"navicon\" onclick=\"\">{{{navIcon}}}\n<nav id=\"toc\">\n<ul>\n<li><strong>{{{linkTo '/' 'Home'}}}</strong></li>\n{{#eachPost~}}\n<li>{{{pageLink}}}</li>\n{{/eachPost~}}\n</ul>\n</nav>\n</div>\n\n<div id=\"title\" {{{title-style}}}>\n  <div class=\"title\">{{title}}</div>\n  <div class=\"subtitle\">{{subtitle}}</div>\n</div>\n\n<div id=\"main\" onclick=\"\">\n<div id=\"content\">\n\n{{{renderPage}}}\n\n<div id=\"credit\">{{{credit}}}</div>\n\n</div>\n</div>\n"},{"path":"/post.hbs","text":"<span class=\"permalink\">{{{permaLink}}}</span><span class=\"date\">{{date}}</span>\n\n<div {{{style}}}>\n{{{html}}}\n</div>\n\n{{#eachFragment '#post'}}\n<div {{{style}}}>\n{{{html}}}\n</div>\n{{/eachFragment}}\n"}]},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-seo/src","fragmentDelim":1,"_pkg":"pub-pkg-seo","name":"src","type":"FILE","watch":false,"files":[{"path":"/pages","text":"---- /robots.txt ----\nnocrawl:1\nnolayout:1\ntemplate:robots.txt\naccess:everyone\n\n---- /sitemap.xml ----\nnocrawl:1\nnolayout:1\ntemplate:sitemap.xml\naccess:everyone\n"},{"path":"/robots.txt.hbs","text":"user-agent: *\n{{#ifOption 'noRobots'}}\ndisallow: /\n{{else}}\nallow: /\n{{/ifOption}}\n"},{"path":"/sitemap.xml.hbs","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n{{#eachPage~}}\n<url><loc>{{fqurl}}</loc></url>\n{{/eachPage}}\n</urlset>\n"}]},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-editor/src","writable":true,"_pkg":"pub-pkg-editor","name":"_src","type":"FILE","tmp":"tmp/_src","watch":false,"fragmentDelim":true,"files":[{"path":"/pub-editor-doclayout.hbs","text":"<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"UTF-8\">\n<title>pub {{name}}</title>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<link rel=\"stylesheet\" href=\"{{relPath}}/pub/css/editor.css\">\n</head>\n<body>\n{{{renderPage}}}\n<script src=\"{{relPath}}/js/jquery-1.12.4.min.js\"></script>\n</body>\n</html>\n"},{"path":"/pub-editor-help.md","text":"---- /pub-editor-help ----\nname: pub Editor\ndoclayout: pub-editor-doclayout\nnopublish: 1\n\n\n## Editing pages with pub server\n\n- **try it out!**  -- you can [edit this page](/pub/editor-help).\n\n- Click on the (E) button (ony visible for Editors) at the top right on the staging server, to open any page for editing in Composer.\n\n- Once open, the (E) changes to an (C) which closes the Composer and returns to normal staging website navigation.\n\n- In composer mode, there are two panes.  \n  The markdown editor on the left  \n  And the preview pane on the right\n\n- You can drag the little round dragger at top of the line separating the panes left and right. Your settings will be remembered across sessions.\n\n- The preview pane can be used to navigate the website, select other pages for editing, and display, in real-time, any changes you make in the editor on the left.\n\n- The website in the preview pane should look and behave just like the normal (staging) website. Click on links to navigate, and even use your browser's back button. The only exceptions are pages which require information from the server e.g. the thank-you pages after submitting a registration or an info request.\n\n- The composer can also be opened by prepending `pub/` to the beginning of the path in the url. E.g. to open this page in composer, go to [/pub/editor-help](/pub/editor-help)\n\n- sometimes if the network is slow, the composer won't work the first time -- navigating to another page by clicking somewhere in the preview should fix it.\n\n## Modifying pages\n\n- click on the editor button (E) at the top right, and wait a second or two for the markdown to appear in the left pane.\n\n- the header section at the top of the markdown in the left pane contains \"meta\" information like (most importantly) the `page:` which is the url path to the page, and the `template:` which defines the type of page.\n\n- below the header section is the \"main\" markdown content for the page.\n\n- edit away..., You should see your changes immediately reflected in the preview pane on the right.\n\n- All changes are saved automatically to the server (no need to click on any save button) but nothing is published to the www website. \n\n- Click on the close button (C) at the top right, to close the composer, and see the change on the staging site.\n\n## Committing changes\n\n- At any time you can click on the âœ“ (check symbol) at the top of the left pane.\n\n- you should see a list of all the updated pages or fragments which have not yet been committed.\n\n- Hovering over any one of the updates in the list should show you a bit of what changed (in case you forgot).\n\n- Clicking on one of the updates in the list will present a confirmation prompt. Click on OK to commit or Cancel to go back without comitting.\n\n- Once you commit, it will take a few minutes for the main www server to refresh its pages with those changes (right now the interval is set to 5 min).\n\n## Modifying fragments\n\n- Content like the banner ads, and the bio's on the location pages and the staff page, lives in \"fragments\". These have a `fragment:#fragment-name` header instead of a `page:path` header.\n\n- to edit this, navigate to the page, then click on the **âœ** fragment selection pointer (little hand) at the top right and then click anywhere inside the fragment that you want to edit in the preview pane on the right. The markdown for the selected fragment should appear on the left.\n\n## Adding or removing pages\n\n- Contact your web admin for help adding new pages, or changing page urls.\n\n## Uploading images\n\n- For best results, images should be sized at 2x the desired pixel width and height and then given explicit width and height dimensions (1/2 of the actual) when inserted. See [composer markdown extensions](#composer-markdown-extensions)  below for more details\n\n- To upload an image, click on the â˜° button at the top left. This will open a little uploader form which you can browse your file system for the image that you want to upload (drag and drop not supported yet sorry). After uploading you should see the image below the form together with a markdown snippet for the image, and the image URL.\n\n- The uploader will rename the image file with just lowercase and hyphens, but please use meaningful file names for your images before uploading them, so that someone looking at all the image files, can recognize what it is.\n\n- Usually you will copy the markdown snippet and paste it somewhere into the markdown editor. Note that inside markdown, you don't need the fully qualified image source url, just start the image url or link with `/images/...`\n\n# Markdown\n\nEditor supports [github flavored markdown](https://help.github.com/articles/github-flavored-markdown/).\n\n### Paragraphs and line breaks\nSimply leave a blank line between paragraphs. To force a line break put 2 spaces at the end.  \nlike so.  \ndone.\n\n### links and images\n\nIn general links have the form `[text](url \"title\")`\n\nLinks within the website don't require any [text] if the page has a name e.g. `[](/contact-fmc)` will become [](/contact-fmc).\n\nAlso, if the url starts with http:// or https:// and you want to show the url in the resulting page, you can just include it inline in the text without wrapping it in `[]()`\n\nMarkdown for images looks just like markdown for links with a `!` in front  \nE.g. `![jurgen's kids a few years ago](/images/gmail-logo-1.gif)`  \n![jurgen's kids a few years ago](/images/gmail-logo-1.gif)\n\nIf you want an image which links to another page, you put the image markdown inside the text part of the link markdown like this.  \n`[![Image](image-src)](link-url)`\n\n### Editor markdown extensions\nMost of these extensions work by recognising patterns in the `[]( \"title\")` part of the link markdown i.e the part in `\"\"` after the url.\n\n- Use `^` to force a link to open in a new window E.g. `[](/help \"^\")` [](/help \"^\")\n\n- Note: \"Fully qualified\" links (which include `https://servername/...`) automatically open in a new tab/window. E.g.  \n`[google](https://www.google.com)` opens a new window on\n[google](https://www.google.com)\n\n- The link title part of the image markdown can be used for `WxH` sizing or `name=value` attributes. E.g.  \n`\"align=right\"` ![](/images/gmail-logo-1.gif \"align=right\")  \n`\"width=32\"` ![](/images/gmail-logo-1.gif \"width=32\")  \n`\"12x12\" `![](/images/gmail-logo-1.gif \"12x12\")\n\n### Markdown inline formatting\n- **BOLD**\n- *ITALIC*\n- ***BOLD AND ITALIC***\n- ~~STRIKE-THRU~~\n- `quoted-text with <> tags`\n\n\n    quoted\n    paragraph\n    spanning multiple lines\n\n\n# h1 level heading\n## h2 level heading\n### h3 level heading\n#### h4 level heading\n\n### lists\n\n- bullet list\n- bullet list\n  - sub-bullet\n  - sub-bullet\n    - sub-sub-bullet\n    - sub-sub-bullet\n\n\n1. numbered list\n   1. sub-point\n   2. sub-point\n    3. sub-sub-point\n2. numbered list\n3. numbered list\n\n* * * \nUse `* * *` on a separate line for horizontal rules \n\n---\n\n### tables\n\nGFM tables use `|` to separate columns\n\n    | Left-Aligned  | Center Aligned  | Right Aligned |\n    | :------------ |:---------------:| -----:|\n    | col 3 is      | some wordy text | $1600 |\n    | col 2 is      | centered        |   $12 |\n    | zebra stripes | are neat        |    $1 |\n\nbecomes...\n\n| Left-Aligned  | Center Aligned  | Right Aligned |\n| :------------ |:---------------:| -----:|\n| col 3 is      | some wordy text | $1600 |\n| col 2 is      | centered        |   $12 |\n| zebra stripes | are neat        |    $1 |\n\n...\n---\n\nHere's a [Markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) with more information about things like tables.\n\nAnd here's some [more background](/help/markdown-motivation) on the motivations behind markdown\n\n![](/images/2016-05-07-13.43.19.jpg \"width=100\")\n"},{"path":"/pub-editor-updates.hbs","text":"<div class=\"editorupdates\">\n<h3>{{name}}</h3>\n<ul class=\"difflist\">\n{{#each diffs}}\n<li title=\"Click to open in editor.{{difftext}}\" data-file=\"{{file}}\" data-href=\"{{diffpage}}\" class=\"difflistitem\">\n  <span class=\"diffrevert\" title=\"Click to revert changes (will trigger editor reload.)\">ð„‚</span>\n  <span class=\"diffcommit\" title=\"Click to commit and publish changes.\">âœ”ï¸Ž</span>\n  {{difffragments}}\n</li>\n{{/each}}\n</ul>\n</div>\n"},{"path":"/pub-editor-upload.hbs","text":"<div class=\"upload\">\n<h3>{{name}}</h3>\n<form method=\"POST\" enctype=\"multipart/form-data\" action=\"/admin/pub-editor-upload\">\n{{{html}}}\n</form>\n{{#eachUpload}}<pre>![](/images/{{{name}}}){{s3Err}}\n</pre>\n<img src=\"{{opts 'fqImages'}}/images/{{name}}\"><br>\n{{opts 'fqImages'}}/images/{{name}}\n{{/eachUpload}}\n</div>\n"},{"path":"/pub-editor.hbs","text":"<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"UTF-8\">\n<title>pub editor</title>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<link rel=\"stylesheet\" href=\"{{relPath}}/pub/css/editor.css\">\n<link rel=\"stylesheet\" href=\"{{relPath}}/pub/humane-js/flatty.css\">\n</head>\n<body>\n<div class=\"handle leftright\"></div>\n\n<div class=\"outer col\">\n\n<div class=\"header row\">\n  <div class=\"menubutton col clickable\" title=\"upload images\"> â˜° </div>\n  <div class=\"commitbutton col clickable\" title=\"List changes and commit\"> âœ”ï¸Ž </div>\n  <div class=\"name col\"></div>\n  <div class=\"editbutton col clickable\" title=\"fragments\"> âœ </div>\n  <div class=\"helpbutton col clickable\" title=\"help\"> ? </div>\n</div>\n\n<div class=\"main row\">\n  <div class=\"editorpane left col\">\n    <div class=\"updateslist row shadow\"></div>\n    <iframe class=\"uploadsform row shadow\" src=\"{{relPath}}/admin/pub-editor-upload\"></iframe>\n    <textarea class=\"editor row\"></textarea></div>\n  <div class=\"previewpane right col\"><iframe class=\"preview\" src=\"{{relPath}}{{route}}\"></iframe></div>\n</div>\n\n</div>\n<script>window.pubRef = {\"href\":\"{{editorPrefix}}/\",\"relPath\":\"{{relPath}}\"};</script>\n<script src=\"{{relPath}}/js/jquery-1.12.4.min.js\"></script>\n<script src=\"{{relPath}}/pub/js/editor-ui.js\"></script>\n<script src=\"{{relPath}}/pub/_generator.js\"></script>\n\n</body>\n</html>\n"},{"path":"/pub-editor.md","text":"---- /pub/ ----\nname: pub-server\ndoclayout: pub-editor\nnocrawl: 1\n\n\n---- /admin/pub-editor-upload ----\nname: Upload Images\ntemplate: pub-editor-upload\ndoclayout: pub-editor-doclayout\npostandget: 1\nnocrawl: 1\n\n[?file?](upload)\n[?submit? Upload](-)  \n[Markdown will appear below. ?label?](- \"instructions\")\n"}]}],"staticPaths":[{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/images","route":"/images","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/favicon.ico","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/CNAME","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/.nojekyll","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/cloudflare-pages-test-log-1.txt","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/vercel.json","watch":{}},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-jquery/js/jquery-1.12.4.min.js","route":"/js","inject":true,"maxAge":"30d","_pkg":"pub-pkg-jquery","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog/css/pubblog.css","route":"/css","inject":true,"_pkg":"pub-theme-pubblog","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog/images/default.jpg","route":"/images","_pkg":"pub-theme-pubblog","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-open-sans/open-sans.css","route":"/css","inject":true,"_pkg":"pub-pkg-font-open-sans","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/open-sans-fontface/fonts","route":"/css/fonts","glob":"**/*.{eot,woff,woff2}","maxAge":"1000d","_pkg":"pub-pkg-font-open-sans","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-awesome/fonts","route":"/fonts","_pkg":"pub-pkg-font-awesome","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-awesome/css/font-awesome.css","route":"/css","inject":true,"_pkg":"pub-pkg-font-awesome","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-highlight/css/highlight-11.4.0-github.css","route":"/css","inject":true,"maxAge":"30d","_pkg":"pub-pkg-highlight","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-highlight/js/highlight-11.4.0.min.js","route":"/js","inject":true,"maxAge":"30d","_pkg":"pub-pkg-highlight","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-highlight/js/pub-pkg-highlight.js","route":"/js","inject":true,"maxAge":"30d","_pkg":"pub-pkg-highlight","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-editor/static/css","route":"/pub/css","_pkg":"pub-pkg-editor","watch":false},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/humane-js/themes/flatty.css","route":"/pub/humane-js","_pkg":"pub-pkg-editor","watch":false}],"outputs":[{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/out","relPaths":true,"name":"out","output":true,"writable":true,"type":"FILE","tmp":"tmp/out","watch":{}}],"browserScripts":[{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-editor/client/editor-ui.js","route":"/pub/js/editor-ui.js","_pkg":"pub-pkg-editor"},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-preview/pub-preview.js","route":"/pub/js/pub-preview.js","_pkg":"pub-pkg-editor"}],"generatorPlugins":[{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-editor/generator-plugin.js","_pkg":"pub-pkg-editor"},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-seo/generator-plugin.js","_pkg":"pub-pkg-seo"},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-awesome/generator-plugin.js","_pkg":"pub-pkg-font-awesome"},{"path":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog/plugins/generator-plugin.js","_pkg":"pub-theme-pubblog"}],"serverPlugins":[],"injectCss":[{"path":"/css/pubblog.css"},{"path":"/css/open-sans.css"},{"path":"/css/font-awesome.css"},{"path":"/css/highlight-11.4.0-github.css"}],"injectJs":[{"path":"/js/jquery-1.12.4.min.js"},{"path":"/js/highlight-11.4.0.min.js"},{"path":"/js/pub-pkg-highlight.js"},{"path":"/pub/pub-ux.js"}],"pkgs":[{"path":"pub-pkg-jquery","pkgJson":{"name":"pub-pkg-jquery","version":"1.12.4","description":"pub-server package to inject jQuery","main":"pub-config.js","dependencies":{},"keywords":["publish","jquery"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-jquery.git"},"author":"jurgen leschner","license":"MIT"},"pkgName":"pub-pkg-jquery","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-jquery"},{"path":"pub-theme-pubblog","pkgJson":{"name":"pub-theme-pubblog","version":"1.4.0","description":"pub-server blogging theme","main":"pub-config.js","dependencies":{},"files":["css/","plugins/","templates/","images/","pub-config.js"],"keywords":["publish","markdown","pubblog"],"repository":{"type":"git","url":"git://github.com/jldec/pub-theme-pubblog.git"},"author":"JÃ¼rgen Leschner","license":"MIT"},"pkgName":"pub-theme-pubblog","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog"},{"path":"pub-pkg-font-open-sans","pkgJson":{"name":"pub-pkg-font-open-sans","version":"1.5.1","description":"pub-server package for publishing with the Open Sans font","main":"pub-config.js","dependencies":{"open-sans-fontface":"^1.4.0"},"devDependencies":{"pub-pkg-show-font":"^1.1.1"},"files":["pub-config.js","open-sans.css"],"keywords":["pub-server","publish","markdown","font","open-sans"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-font-open-sans.git"},"scripts":{},"author":"JÃ¼rgen Leschner","license":"MIT AND Apache-2.0"},"pkgName":"pub-pkg-font-open-sans","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-open-sans"},{"path":"pub-pkg-font-awesome","pkgJson":{"name":"pub-pkg-font-awesome","version":"3.0.5","description":"Use Font Awesome v4.7.0 glyphs from pub-server markdown","main":"pub-config.js","dependencies":{},"devDependencies":{},"files":["css/","fonts/","pub-config.js","generator-plugin.js"],"keywords":["publish","markdown","icon","font","awesome"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-font-awesome.git"},"scripts":{"readme":"pub","docedit":"pub docs-src","docout":"pub -O docs-src","docview":"pub -S docs"},"author":"Font Awesome by Dave Gandy - http://fontawesome.io","license":"MIT AND OFL-1.1"},"pkgName":"pub-pkg-font-awesome","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-font-awesome"},{"path":"pub-pkg-seo","pkgJson":{"name":"pub-pkg-seo","version":"1.2.2","description":"pub-server package for generating robots.txt and sitemap.xml","main":"pub-config.js","dependencies":{},"devDependencies":{"eslint":"^8.7.0","pub-generator":"^4.3.2","pub-resolve-opts":"^1.9.4","pub-src-fs":"^2.1.3","tape":"^5.4.1"},"files":["pub-config.js","generator-plugin.js","src/"],"keywords":["publish","seo","robots","sitemap"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-seo.git"},"scripts":{"test":"eslint . && tape test/norobots/norobots.test.js && tape test/yesrobots/yesrobots.test.js && tape test/canonical/canonical.test.js"},"author":"JÃ¼rgen Leschner","license":"MIT"},"pkgName":"pub-pkg-seo","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-seo"},{"path":"pub-pkg-highlight","pkgJson":{"name":"pub-pkg-highlight","version":"11.4.0","description":"pub-server package for injecting highlight.js","main":"pub-config.js","dependencies":{},"files":["css/","js/","pub-config.js","LICENSE-HIGHLIGHT"],"keywords":["publish","syntax","highlight"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-highlight.git"},"author":"JÃ¼rgen Leschner","license":"MIT"},"pkgName":"pub-pkg-highlight","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-highlight"},{"path":"pub-pkg-editor","pkgJson":{"name":"pub-pkg-editor","version":"2.0.4","description":"simple markdown editor for pub-server","main":"pub-config.js","dependencies":{"humane-js":"^3.2.2","pub-preview":"^1.3.8"},"devDependencies":{},"files":["client/","src/","static/","pub-config.js","generator-plugin.js"],"repository":{"type":"git","url":"git://github.com/jldec/pub-pkg-editor.git"},"author":"JÃ¼rgen Leschner","license":"MIT"},"pkgName":"pub-pkg-editor","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-pkg-editor"}],"docTitle":"jldec.uk","appUrl":"https://jldec.uk","canonicalUrl":"https://jldec.me","noRobots":true,"throttleReload":"1s","linkNewWindow":true,"tmp":"./tmp","github":"https://github.com/jldec/cloudflare-pages-test","editorPrefix":"/pub","theme":{"path":"pub-theme-pubblog","pkgJson":{"name":"pub-theme-pubblog","version":"1.4.0","description":"pub-server blogging theme","main":"pub-config.js","dependencies":{},"files":["css/","plugins/","templates/","images/","pub-config.js"],"keywords":["publish","markdown","pubblog"],"repository":{"type":"git","url":"git://github.com/jldec/pub-theme-pubblog.git"},"author":"JÃ¼rgen Leschner","license":"MIT"},"pkgName":"pub-theme-pubblog","dir":"/home/runner/work/cloudflare-pages-test/cloudflare-pages-test/node_modules/pub-theme-pubblog"},"_resolved":true,"production":false,"port":"3001","fqImages":"","staticHost":true}